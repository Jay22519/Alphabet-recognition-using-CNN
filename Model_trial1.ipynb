{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/Aplhabet Recognition/archive/A_Z Handwritten Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_div = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_div = [0 for i in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby(data['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(26) :\n",
    "    data_div[i] = grouped.get_group(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_div[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>...</th>\n",
       "      <th>0.639</th>\n",
       "      <th>0.640</th>\n",
       "      <th>0.641</th>\n",
       "      <th>0.642</th>\n",
       "      <th>0.643</th>\n",
       "      <th>0.644</th>\n",
       "      <th>0.645</th>\n",
       "      <th>0.646</th>\n",
       "      <th>0.647</th>\n",
       "      <th>0.648</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81663</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81664</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81665</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81666</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81667</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82778</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82779</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82780</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82781</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82782</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1120 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  ...  0.639  0.640  \\\n",
       "81663  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "81664  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "81665  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "81666  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "81667  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...   ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "82778  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "82779  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "82780  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "82781  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "82782  8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       0.641  0.642  0.643  0.644  0.645  0.646  0.647  0.648  \n",
       "81663      0      0      0      0      0      0      0      0  \n",
       "81664      0      0      0      0      0      0      0      0  \n",
       "81665      0      0      0      0      0      0      0      0  \n",
       "81666      0      0      0      0      0      0      0      0  \n",
       "81667      0      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "82778      0      0      0      0      0      0      0      0  \n",
       "82779      0      0      0      0      0      0      0      0  \n",
       "82780      0      0      0      0      0      0      0      0  \n",
       "82781      0      0      0      0      0      0      0      0  \n",
       "82782      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[1120 rows x 785 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_div[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_div = [0 for i in range(26)] \n",
    "for i in range(26) :\n",
    "    data_div[i] = data_div[i].sample(n = 75)\n",
    "    y_div[i] = data_div[i]['0']\n",
    "    del data_div[i]['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.10</th>\n",
       "      <th>...</th>\n",
       "      <th>0.639</th>\n",
       "      <th>0.640</th>\n",
       "      <th>0.641</th>\n",
       "      <th>0.642</th>\n",
       "      <th>0.643</th>\n",
       "      <th>0.644</th>\n",
       "      <th>0.645</th>\n",
       "      <th>0.646</th>\n",
       "      <th>0.647</th>\n",
       "      <th>0.648</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81698</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82744</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82404</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82554</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82678</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  0.10  ...  0.639  0.640  \\\n",
       "82145    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "82131    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "81698    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "82744    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "82404    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...    ...    ...   \n",
       "82014    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "82164    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "82554    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "82678    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "82035    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "\n",
       "       0.641  0.642  0.643  0.644  0.645  0.646  0.647  0.648  \n",
       "82145      0      0      0      0      0      0      0      0  \n",
       "82131      0      0      0      0      0      0      0      0  \n",
       "81698      0      0      0      0      0      0      0      0  \n",
       "82744      0      0      0      0      0      0      0      0  \n",
       "82404      0      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "82014      0      0      0      0      0      0      0      0  \n",
       "82164      0      0      0      0      0      0      0      0  \n",
       "82554      0      0      0      0      0      0      0      0  \n",
       "82678      0      0      0      0      0      0      0      0  \n",
       "82035      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[75 rows x 784 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_div[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23185    2\n",
       "24955    2\n",
       "37976    2\n",
       "42797    2\n",
       "23113    2\n",
       "        ..\n",
       "41172    2\n",
       "43476    2\n",
       "29706    2\n",
       "25078    2\n",
       "23536    2\n",
       "Name: 0, Length: 75, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_div[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So we are done with Image Input \n",
    "<br>\n",
    "<h3> Now we have 75 images of image type </h3>\n",
    "<br><br>\n",
    "\n",
    "##### Now we have to perform Data Augmentation to increase image size to 300 for each type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idg() :\n",
    "    idg = ImageDataGenerator(\n",
    "    rotation_range=random.randint(0,10),\n",
    "    zoom_range = random.uniform(0,0.5), \n",
    "    fill_mode='constant', cval=255\n",
    "    )\n",
    "    \n",
    "    return idg \n",
    "\n",
    "def get_idg_no() :\n",
    "    idg = ImageDataGenerator(\n",
    "    rotation_range= 0 \n",
    "    )\n",
    "    \n",
    "    return idg \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "image_data_aug  = [0 for i in range(26)] \n",
    "y_data_aug = [0 for i in range(26)] \n",
    "\n",
    "for i in range(26) :\n",
    "    a ,ya = get_idg_no().flow(data_div[i].values.reshape(len(data_div[i]),28,28,1), y_div[i],batch_size=75).next()\n",
    "    b , yb = get_idg().flow(data_div[i].values.reshape(len(data_div[i]),28,28,1), y_div[i],batch_size=75).next()\n",
    "    #print(a.shape)\n",
    "    a = a.reshape(75,28*28)\n",
    "    b = b.reshape(75,28*28)\n",
    "    a = pd.DataFrame(a)\n",
    "    b = pd.DataFrame(b)\n",
    "    ya = pd.DataFrame(ya)\n",
    "    yb = pd.DataFrame(yb)\n",
    "    #print(a)\n",
    "    aug_2 = (a,b)\n",
    "    y_data_aug[i] = [y_div[i] for j in range(2)]\n",
    "    #print(aug_4_y)\n",
    "    image_data_aug[i] =  pd.concat(aug_2, ignore_index = True)\n",
    "    y_data_aug[i] = pd.concat(y_data_aug[i]  ,ignore_index = True)\n",
    "    #print(y_data_aug[i])\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_data_aug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9    \\\n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "145  255.0  255.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "146  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0    0.0   \n",
       "147  255.0  255.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "148  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "149  255.0  255.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "     ...    774    775    776    777    778    779    780    781    782    783  \n",
       "0    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "145  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  255.0  255.0  \n",
       "146  ...    0.0    0.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "147  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  255.0  255.0  \n",
       "148  ...  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "149  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  255.0  255.0  \n",
       "\n",
       "[150 rows x 784 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data_aug[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      25\n",
       "1      25\n",
       "2      25\n",
       "3      25\n",
       "4      25\n",
       "       ..\n",
       "145    25\n",
       "146    25\n",
       "147    25\n",
       "148    25\n",
       "149    25\n",
       "Name: 0, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_aug[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a903298100>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_plot = image_data_aug[1].loc[1,:]\n",
    "i_plot = np.array(i_plot)\n",
    "plt.imshow(i_plot.reshape(28,28),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a903e99fa0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_plot = image_data_aug[0].loc[0,:]\n",
    "i_plot = np.array(i_plot)\n",
    "plt.imshow(i_plot.reshape(28,28),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So done with Data_Augmenation \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<h4> In the code image_data_aug[i] represents all images of ith alphabet </h4> <br><br>\n",
    "<h4> Similarly y_data_aug[i] represents the value for ith alphabet </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9    \\\n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "145  255.0  255.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "146  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0    0.0   \n",
       "147  255.0  255.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "148  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "149  255.0  255.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "     ...    774    775    776    777    778    779    780    781    782    783  \n",
       "0    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "145  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  255.0  255.0  \n",
       "146  ...    0.0    0.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "147  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  255.0  255.0  \n",
       "148  ...  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "149  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  255.0  255.0  \n",
       "\n",
       "[150 rows x 784 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data_aug[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = [0 for i in range(26)]\n",
    "y_data = [0 for i in range(26)]\n",
    "\n",
    "for i in range(26) :\n",
    "    pd_data[i] = image_data_aug[i]\n",
    "    y_data[i]  = y_data_aug[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat(pd_data)\n",
    "y = pd.concat(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3900 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9    \\\n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "145  255.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "146  255.0    0.0    0.0    0.0    0.0    0.0  255.0  255.0  255.0  255.0   \n",
       "147  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "148  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "149  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0   \n",
       "\n",
       "     ...    774    775    776    777    778    779    780    781    782    783  \n",
       "0    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "145  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  255.0  \n",
       "146  ...  255.0  255.0  255.0  255.0  255.0    0.0    0.0    0.0    0.0  255.0  \n",
       "147  ...  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "148  ...  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "149  ...  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  255.0  \n",
       "\n",
       "[3900 rows x 784 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 25, 25, 25], dtype=int64)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify= y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scaler = MinMaxScaler()\n",
    "# standard_scaler.fit(X_train)\n",
    "\n",
    "# X_train = standard_scaler.transform(X_train)\n",
    "# X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "\n",
    "X_test = X = X_test.reshape(-1,28,28,1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(width,height,channel) :\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (width,height,channel)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.20))\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.20)) \n",
    "    \n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.20)) \n",
    "    \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation = \"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(26, activation = \"softmax\"))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model.compile(optimizer='adamax', loss = 'categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"cp.ckpt\"\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "49/49 [==============================] - 11s 200ms/step - loss: 3.3128 - accuracy: 0.1751 - val_loss: 2.6352 - val_accuracy: 0.1949\n",
      "\n",
      "Epoch 00001: saving model to cp.ckpt\n",
      "Epoch 2/25\n",
      "49/49 [==============================] - 10s 194ms/step - loss: 1.2722 - accuracy: 0.6084 - val_loss: 1.5192 - val_accuracy: 0.6103\n",
      "\n",
      "Epoch 00002: saving model to cp.ckpt\n",
      "Epoch 3/25\n",
      "49/49 [==============================] - 10s 194ms/step - loss: 0.7552 - accuracy: 0.7648 - val_loss: 0.7978 - val_accuracy: 0.8064\n",
      "\n",
      "Epoch 00003: saving model to cp.ckpt\n",
      "Epoch 4/25\n",
      "49/49 [==============================] - 10s 206ms/step - loss: 0.5040 - accuracy: 0.8490 - val_loss: 0.4726 - val_accuracy: 0.8679\n",
      "\n",
      "Epoch 00004: saving model to cp.ckpt\n",
      "Epoch 5/25\n",
      "49/49 [==============================] - 10s 202ms/step - loss: 0.3766 - accuracy: 0.8860 - val_loss: 0.3038 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00005: saving model to cp.ckpt\n",
      "Epoch 6/25\n",
      "49/49 [==============================] - 10s 203ms/step - loss: 0.3057 - accuracy: 0.9077 - val_loss: 0.2228 - val_accuracy: 0.9372\n",
      "\n",
      "Epoch 00006: saving model to cp.ckpt\n",
      "Epoch 7/25\n",
      "49/49 [==============================] - 10s 213ms/step - loss: 0.2428 - accuracy: 0.9349 - val_loss: 0.1736 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00007: saving model to cp.ckpt\n",
      "Epoch 8/25\n",
      "49/49 [==============================] - 10s 200ms/step - loss: 0.1983 - accuracy: 0.9433 - val_loss: 0.1580 - val_accuracy: 0.9590\n",
      "\n",
      "Epoch 00008: saving model to cp.ckpt\n",
      "Epoch 9/25\n",
      "49/49 [==============================] - 10s 201ms/step - loss: 0.1742 - accuracy: 0.9495 - val_loss: 0.1483 - val_accuracy: 0.9526\n",
      "\n",
      "Epoch 00009: saving model to cp.ckpt\n",
      "Epoch 10/25\n",
      "49/49 [==============================] - 10s 204ms/step - loss: 0.1413 - accuracy: 0.9628 - val_loss: 0.1356 - val_accuracy: 0.9577\n",
      "\n",
      "Epoch 00010: saving model to cp.ckpt\n",
      "Epoch 11/25\n",
      "49/49 [==============================] - 10s 201ms/step - loss: 0.1121 - accuracy: 0.9723 - val_loss: 0.1338 - val_accuracy: 0.9590\n",
      "\n",
      "Epoch 00011: saving model to cp.ckpt\n",
      "Epoch 12/25\n",
      "49/49 [==============================] - 10s 204ms/step - loss: 0.1027 - accuracy: 0.9730 - val_loss: 0.1310 - val_accuracy: 0.9538\n",
      "\n",
      "Epoch 00012: saving model to cp.ckpt\n",
      "Epoch 13/25\n",
      "49/49 [==============================] - 10s 204ms/step - loss: 0.0923 - accuracy: 0.9765 - val_loss: 0.1128 - val_accuracy: 0.9603\n",
      "\n",
      "Epoch 00013: saving model to cp.ckpt\n",
      "Epoch 14/25\n",
      "49/49 [==============================] - 10s 200ms/step - loss: 0.0915 - accuracy: 0.9781 - val_loss: 0.1035 - val_accuracy: 0.9667\n",
      "\n",
      "Epoch 00014: saving model to cp.ckpt\n",
      "Epoch 15/25\n",
      "49/49 [==============================] - 11s 221ms/step - loss: 0.0679 - accuracy: 0.9827 - val_loss: 0.0949 - val_accuracy: 0.9679\n",
      "\n",
      "Epoch 00015: saving model to cp.ckpt\n",
      "Epoch 16/25\n",
      "49/49 [==============================] - 10s 207ms/step - loss: 0.0754 - accuracy: 0.9836 - val_loss: 0.0930 - val_accuracy: 0.9667\n",
      "\n",
      "Epoch 00016: saving model to cp.ckpt\n",
      "Epoch 17/25\n",
      "49/49 [==============================] - 10s 201ms/step - loss: 0.0557 - accuracy: 0.9844 - val_loss: 0.0970 - val_accuracy: 0.9667\n",
      "\n",
      "Epoch 00017: saving model to cp.ckpt\n",
      "Epoch 18/25\n",
      "49/49 [==============================] - 10s 208ms/step - loss: 0.0583 - accuracy: 0.9854 - val_loss: 0.0939 - val_accuracy: 0.9654\n",
      "\n",
      "Epoch 00018: saving model to cp.ckpt\n",
      "Epoch 19/25\n",
      "49/49 [==============================] - 10s 207ms/step - loss: 0.0507 - accuracy: 0.9893 - val_loss: 0.0879 - val_accuracy: 0.9756\n",
      "\n",
      "Epoch 00019: saving model to cp.ckpt\n",
      "Epoch 20/25\n",
      "49/49 [==============================] - 10s 202ms/step - loss: 0.0461 - accuracy: 0.9910 - val_loss: 0.0887 - val_accuracy: 0.9705\n",
      "\n",
      "Epoch 00020: saving model to cp.ckpt\n",
      "Epoch 21/25\n",
      "49/49 [==============================] - 10s 208ms/step - loss: 0.0432 - accuracy: 0.9886 - val_loss: 0.0836 - val_accuracy: 0.9705\n",
      "\n",
      "Epoch 00021: saving model to cp.ckpt\n",
      "Epoch 22/25\n",
      "49/49 [==============================] - 11s 224ms/step - loss: 0.0461 - accuracy: 0.9909 - val_loss: 0.0745 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00022: saving model to cp.ckpt\n",
      "Epoch 23/25\n",
      "49/49 [==============================] - 10s 204ms/step - loss: 0.0322 - accuracy: 0.9948 - val_loss: 0.0800 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00023: saving model to cp.ckpt\n",
      "Epoch 24/25\n",
      "49/49 [==============================] - 10s 208ms/step - loss: 0.0263 - accuracy: 0.9977 - val_loss: 0.0726 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00024: saving model to cp.ckpt\n",
      "Epoch 25/25\n",
      "49/49 [==============================] - 10s 209ms/step - loss: 0.0318 - accuracy: 0.9918 - val_loss: 0.0740 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00025: saving model to cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=64, shuffle=True,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unravel_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(y_test) :\n",
    "    ans = [0 for i in range(len(y_test))]\n",
    "    for i in range(len(y_test)) :\n",
    "        index = unravel_index(y_test[i].argmax(), y_test[i].shape)\n",
    "        ans[i] = index[0] \n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 12  2  8  3  6 14 17 24 16 25 19 19 22 14 14 12 16 12  5  2 17  4 16\n",
      "  3  3 24 15 19  2  7  2 23  0 10  0 20 19  1 18 21  5  5  3  8 18 18  4\n",
      "  8 24 18 15  1  8 21  8  0 18 19 18 25 17 25 15 23 20 24 22 12 16 13  7\n",
      " 21  3 14 14  9 13  2 21 16 21  0  2  3  1 16  0  9  9  7 22 15  9  3  7\n",
      " 20 12 12  6 23 12 18  6 11 10 11  6 15  5 13  2 11  0 22 16 16  3 13 22\n",
      " 15 11  0  6 15 14  6 24 23 10  2  3 17 13  0  0 13 14 18  5  0  3  7  8\n",
      " 25  4 24 23 23 20 23 22 15 22 12 10  3  5  6  9 24 20 11  4 19 12  1 18\n",
      " 14 17 11 19 18 15 21  5  0  7 15 25 11 21 16 21 22 24 25 15 14 22  0 11\n",
      "  8 19 12 17 21  7 22 17 18 11  2 14  2  9  8  1 22 14 22 17  0  6 21 17\n",
      " 19  2 24 10 23  1 21 15  1 15 16  9 10 10 18  7  5  9  8 24 14 16 18 19\n",
      " 12 25 18 10 11 19  7  2 19  2  3 20 23 20 19 25  6  3 25 21  5  4  8 23\n",
      "  6  1 11  8  4 14  8 11 25 23  6 11 19 23 20 10 13 14  1  0  8  6 24 10\n",
      " 18 12  9 16  3 18 22 13  4 12 10  1 18 24 21  5 22  1 16 14 19  7  9  6\n",
      " 18  6  9  0  1 11 23  4 23  3 25  5 21  8  3 12 19 14 19 12  3 23 24  7\n",
      " 23 23  9  6  3 19 13 11  8 23 11  4 17  4 20  7 20 15 21 20 12  1  8  8\n",
      "  7  5  8 21 11 10  2  2 24  6 12  5 12 11  4 22 17 14 25  7 15 25 13  6\n",
      "  8 12  9  0 11 11 11  5  2  9 11 15  0  4  5  7 19 16  0 12 24 13 15  7\n",
      " 13 10 21 20 19 10 17 16 20 18 11  4 10  5 12  1 19 22 18 17  9 25 20  5\n",
      " 17 19 25 11  3 14 24  1  5  7 10 22 13 23  0 11 22 25 25 22 10 10  4  0\n",
      " 15 19 24  1 21 10 16  8  0 17 22 25 25 23  3 13 18  9  1 20 20  4 13  0\n",
      " 19  3  3 20 18  4 24 20  5  9 17 18  6 21  9  1 12  5  0  6  3 10  2 22\n",
      " 24  5  1 24 20 14 17  0 23 15 19  6 10  1 22 11  0 19  2  6  1 16  7  2\n",
      " 20 13  3 15  3  4 20  3 15  5  0 21  4  4 25  9 14 21 15 19  7 13 24 13\n",
      " 24  4  4 22 12 12 17 14 10 20  2  9  0 25 17  0 22 13  4 25 21  7 25  4\n",
      " 11  2 23  9 22  3 20  5  0  8 16 17  4 16 21 25  5 17 21 12  6  8 17 21\n",
      "  6  3 25  8 18 19 22 16  2 18 23  8 24  9  2  7 22  2  8  5 21 25 20  1\n",
      " 11 10 12 12 13 16 16 19 10 15  7 10 18 24  8  5 11 16  7  6 14  5  2 17\n",
      " 19 16 19 15 23  3  3  3  4 14  8 23 18  4 12  6  0 22 21 17 16 24 11  8\n",
      " 21 10  4  6  5 13 20 18 17  2  2 15  2 23  9 12 18  1 17  7  9 13  3 25\n",
      " 24  6  9  0 15  4  6  6 15  9 24 25  7 12 24 16 17 18  8  6 17 23 20  4\n",
      " 10 11 25 13 22 22 10  1 24 22 25  1 22 20  1 12 21  1 25 10 20 18  3 20\n",
      " 23 21  5  1 16 20  7 13 14 25  7 13  6 15 14  7 16  5 24  4 13  3 13  9\n",
      "  7  9 13 17  7 14 14  4 13  4 15 15]\n",
      "[9, 12, 2, 8, 3, 6, 14, 17, 24, 16, 25, 19, 19, 22, 14, 14, 12, 16, 12, 5, 2, 17, 4, 16, 3, 16, 24, 15, 19, 2, 7, 2, 23, 0, 10, 0, 20, 19, 1, 18, 21, 5, 5, 3, 8, 18, 18, 4, 8, 24, 18, 15, 1, 8, 21, 8, 0, 18, 19, 18, 25, 17, 25, 15, 23, 20, 24, 22, 12, 16, 13, 7, 21, 3, 14, 14, 9, 13, 2, 21, 16, 21, 0, 2, 3, 1, 16, 0, 9, 9, 7, 13, 15, 9, 3, 7, 20, 12, 12, 6, 23, 12, 18, 6, 11, 10, 11, 6, 15, 5, 13, 2, 11, 0, 22, 16, 16, 3, 13, 22, 15, 11, 0, 6, 15, 14, 6, 24, 23, 10, 2, 16, 17, 13, 0, 0, 13, 14, 18, 5, 0, 3, 7, 8, 25, 4, 24, 23, 23, 20, 23, 22, 15, 22, 12, 10, 3, 5, 6, 9, 24, 20, 11, 4, 19, 12, 1, 18, 14, 17, 11, 19, 18, 15, 21, 5, 0, 7, 15, 25, 11, 21, 16, 21, 22, 24, 25, 15, 14, 22, 0, 11, 8, 19, 12, 17, 21, 7, 22, 17, 18, 11, 2, 14, 2, 9, 8, 1, 22, 14, 22, 17, 0, 6, 21, 17, 19, 2, 24, 10, 23, 1, 21, 15, 1, 15, 16, 9, 10, 10, 18, 7, 5, 9, 8, 24, 14, 16, 18, 19, 12, 25, 18, 10, 11, 19, 7, 2, 19, 2, 3, 20, 23, 20, 8, 25, 6, 14, 25, 21, 5, 4, 8, 23, 6, 1, 11, 8, 4, 14, 8, 11, 1, 23, 6, 11, 19, 23, 23, 10, 13, 14, 1, 0, 8, 6, 24, 10, 18, 12, 9, 16, 3, 18, 22, 13, 4, 12, 10, 1, 18, 24, 21, 5, 22, 1, 16, 14, 19, 7, 9, 6, 18, 6, 9, 0, 1, 11, 23, 2, 23, 3, 25, 5, 21, 8, 3, 12, 19, 14, 19, 12, 3, 23, 24, 7, 23, 23, 9, 6, 3, 19, 13, 11, 8, 23, 11, 4, 17, 4, 20, 7, 20, 15, 21, 20, 12, 1, 8, 8, 7, 5, 8, 21, 11, 10, 2, 2, 24, 6, 12, 5, 12, 11, 25, 22, 17, 14, 25, 7, 15, 25, 13, 6, 8, 12, 9, 0, 11, 11, 2, 5, 2, 9, 11, 15, 0, 4, 5, 7, 19, 16, 0, 12, 24, 13, 15, 7, 13, 10, 21, 20, 19, 10, 17, 16, 20, 18, 11, 4, 10, 5, 12, 1, 19, 22, 18, 17, 9, 25, 20, 5, 17, 19, 25, 11, 3, 14, 24, 1, 5, 7, 10, 22, 13, 23, 0, 11, 22, 25, 25, 22, 10, 10, 4, 0, 15, 19, 24, 1, 21, 10, 16, 8, 0, 17, 22, 25, 25, 23, 3, 13, 18, 9, 1, 20, 20, 4, 13, 0, 19, 3, 3, 20, 18, 4, 24, 20, 5, 9, 17, 18, 6, 21, 9, 1, 12, 5, 0, 6, 3, 10, 2, 22, 24, 5, 1, 24, 20, 14, 17, 0, 23, 15, 19, 6, 10, 1, 22, 11, 0, 19, 2, 15, 1, 16, 7, 2, 20, 13, 14, 15, 3, 4, 20, 14, 15, 5, 0, 21, 4, 4, 25, 9, 14, 21, 15, 19, 7, 13, 24, 13, 24, 4, 4, 20, 12, 10, 17, 14, 10, 20, 2, 9, 17, 25, 17, 0, 22, 13, 4, 25, 21, 7, 23, 4, 11, 2, 23, 9, 22, 3, 20, 5, 0, 8, 16, 17, 4, 16, 21, 1, 5, 17, 21, 12, 6, 23, 17, 21, 6, 3, 25, 8, 18, 19, 22, 16, 2, 18, 23, 8, 24, 9, 2, 7, 22, 2, 8, 5, 21, 25, 20, 1, 11, 10, 12, 12, 13, 16, 16, 19, 10, 15, 7, 10, 18, 24, 8, 5, 11, 16, 7, 6, 14, 5, 2, 17, 19, 16, 19, 15, 23, 3, 3, 3, 4, 14, 8, 23, 18, 4, 12, 6, 0, 22, 21, 17, 16, 24, 11, 8, 21, 10, 4, 6, 5, 13, 20, 18, 17, 2, 2, 15, 2, 23, 9, 12, 18, 1, 17, 7, 9, 13, 3, 25, 24, 6, 9, 0, 15, 4, 6, 6, 15, 9, 24, 25, 7, 12, 24, 16, 17, 18, 8, 6, 17, 23, 20, 4, 10, 11, 25, 13, 22, 22, 10, 1, 24, 22, 25, 1, 22, 20, 1, 12, 21, 1, 25, 10, 20, 9, 3, 20, 3, 21, 5, 1, 16, 20, 7, 13, 14, 25, 7, 13, 6, 15, 14, 7, 16, 5, 24, 4, 13, 3, 13, 9, 7, 9, 13, 17, 7, 14, 14, 4, 13, 4, 8, 15]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.predict_classes(X_test))\n",
    "print(max_index(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if pre trained saved is working correctly or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JPG\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1313: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Some objects had attributes which were not restored:\n    <tf.Variable 'conv2d_114/kernel:0' shape=(5, 5, 1, 32) dtype=float32, numpy=\narray([[[[-5.14483005e-02, -5.09646162e-02,  2.01726556e-02,\n           7.85938650e-02,  3.67258340e-02, -7.30120391e-03,\n          -5.93751147e-02,  7.33189285e-02, -8.31270590e-02,\n          -3.88594344e-02,  3.89861465e-02,  1.62232667e-02,\n          -6.72789067e-02, -7.12333396e-02, -3.41200717e-02,\n           6.31483197e-02,  4.38336730e-02,  7.46641606e-02,\n          -8.36632103e-02,  5.37628382e-02, -4.50131372e-02,\n           2.38340348e-03, -5.71911484e-02, -1.40122622e-02,\n          -3.33793201e-02,  7.01417178e-02,  5.02503514e-02,\n           2.21611410e-02, -7.90058076e-03, -1.52141675e-02,\n           5.55222481e-02, -2.85171717e-02]],\n\n        [[-1.67478845e-02, -2.93735936e-02, -6.30529225e-02,\n          -4.31816764e-02,  8.84154439e-03,  1.84094086e-02,\n          -7.64121637e-02, -7.32967705e-02,  4.75377440e-02,\n          -4.62013036e-02, -6.51193634e-02, -4.07987162e-02,\n          -2.61693075e-02, -3.33856642e-02,  8.99508595e-05,\n           4.15581316e-02,  7.23612159e-02,  7.55306333e-03,\n          -6.76482022e-02, -1.72045901e-02, -6.42681047e-02,\n           2.92497873e-03,  6.99493438e-02, -5.57070673e-02,\n          -8.36358592e-02, -4.75126803e-02, -1.41796172e-02,\n          -4.88877818e-02, -8.21464509e-02,  8.44985396e-02,\n           6.94993436e-02,  5.51679283e-02]],\n\n        [[ 6.76997751e-03,  1.82163268e-02, -6.91296458e-02,\n          -3.90797965e-02, -1.78025216e-02,  3.28022987e-03,\n          -5.26482202e-02, -7.56224319e-02,  8.05902332e-02,\n          -7.35237598e-02,  8.31596553e-04,  5.22324145e-02,\n          -3.41160037e-02, -2.97664106e-03,  9.39517468e-03,\n          -3.05165388e-02,  8.24845731e-02,  5.36263287e-02,\n           6.09951168e-02, -6.98284507e-02,  3.22947726e-02,\n           7.79469311e-03, -5.61252013e-02,  2.67461166e-02,\n           7.04264045e-02,  5.80172539e-02,  3.37229371e-02,\n           7.27994442e-02,  3.80280018e-02, -6.62386417e-04,\n           2.43976638e-02, -4.94443402e-02]],\n\n        [[ 4.94262874e-02, -2.59392075e-02, -7.12682679e-02,\n           5.91860265e-02,  7.99528509e-02, -6.97373599e-02,\n           1.65382549e-02,  5.92710525e-02, -5.18307760e-02,\n           7.09618032e-02,  7.42624998e-02, -6.47425056e-02,\n          -1.34686530e-02, -4.05449681e-02, -1.62181035e-02,\n          -8.22874904e-03,  5.15605211e-02,  1.91978961e-02,\n          -8.27538595e-02,  2.23291069e-02, -7.14225471e-02,\n           1.30284354e-02,  5.53124994e-02,  5.38352728e-02,\n          -4.44851033e-02,  4.44162339e-02,  1.06921792e-03,\n          -1.26556829e-02,  5.21764010e-02,  5.56270033e-03,\n           2.45717168e-03, -4.56350856e-02]],\n\n        [[-7.69578442e-02,  7.14776367e-02, -6.31289408e-02,\n          -8.35320875e-02, -6.34422451e-02,  6.78313971e-02,\n           4.53344285e-02, -1.02100596e-02, -6.93563074e-02,\n          -2.75498591e-02,  3.55297774e-02,  8.39604288e-02,\n          -7.61588439e-02,  5.95747679e-03,  5.80256879e-02,\n           3.34110633e-02, -2.87995487e-02,  3.92788872e-02,\n           6.50151670e-02, -8.26600865e-02,  1.52926743e-02,\n          -5.40460311e-02, -2.78106332e-03,  6.46309257e-02,\n          -3.56427059e-02, -7.84478411e-02,  1.69967338e-02,\n           6.80284500e-02,  3.58877927e-02,  9.83889401e-03,\n           4.42317426e-02,  3.61514837e-02]]],\n\n\n       [[[-5.19356504e-02, -5.79366088e-03, -3.06328833e-02,\n           5.90210855e-02,  1.16472170e-02, -1.07601956e-02,\n          -6.21170402e-02, -1.77982748e-02, -8.25423449e-02,\n          -6.54612556e-02, -2.31650956e-02,  3.84567678e-02,\n          -3.18618752e-02, -7.05977231e-02, -6.90786913e-02,\n          -7.82714412e-02, -2.62098722e-02,  3.04181501e-02,\n          -5.22549897e-02,  5.11376858e-02, -1.07261762e-02,\n           8.45154971e-02,  7.62859434e-02, -6.10446185e-03,\n          -9.54426825e-04,  5.56863844e-03,  5.31065166e-02,\n           3.66170928e-02,  6.53384626e-02,  4.05988842e-02,\n           5.02419621e-02,  2.33724862e-02]],\n\n        [[ 4.98488098e-02,  3.64903361e-03,  2.66331509e-02,\n           4.67141122e-02, -4.99502905e-02,  1.80221573e-02,\n          -6.75055385e-02, -1.90874338e-02, -6.09182417e-02,\n          -3.02487016e-02,  1.73180625e-02,  7.86809325e-02,\n          -6.30562752e-03,  1.36207193e-02,  4.15116698e-02,\n           1.55274123e-02, -3.40723097e-02, -4.50498611e-03,\n          -6.37702644e-02, -3.30978408e-02,  5.30916899e-02,\n           6.31577075e-02, -3.69128510e-02, -5.05554453e-02,\n          -2.46887021e-02, -2.89748609e-03, -2.89413258e-02,\n          -1.26375854e-02, -3.80911119e-02, -6.40609413e-02,\n          -2.64168344e-02, -4.76165190e-02]],\n\n        [[ 3.13770249e-02, -3.91848944e-02, -8.34010392e-02,\n          -6.69360608e-02,  7.36917108e-02,  7.12070614e-02,\n           1.68437064e-02,  1.21146366e-02,  2.05407888e-02,\n           5.39576858e-02,  3.48509401e-02, -7.42598325e-02,\n          -1.48426369e-02,  1.29753500e-02, -4.52577360e-02,\n          -7.17102364e-02, -2.65413933e-02,  1.63334236e-02,\n           4.20787632e-02,  8.54030252e-03, -8.04318637e-02,\n          -6.47110865e-02, -2.67023034e-02,  6.85000718e-02,\n          -4.32888493e-02,  4.24836427e-02, -3.35543603e-03,\n          -9.29804891e-03,  5.04013002e-02, -1.32977962e-02,\n          -8.15028697e-02, -9.92459059e-03]],\n\n        [[-2.67780200e-02,  3.68281230e-02, -6.02224693e-02,\n          -3.28734927e-02, -3.49213295e-02, -1.93966255e-02,\n          -1.82527006e-02,  3.07456851e-02, -3.62134762e-02,\n          -1.32801682e-02, -7.43378922e-02,  4.78242934e-02,\n          -3.74347009e-02, -3.73389348e-02, -1.35883689e-03,\n           6.66925609e-02, -3.24662030e-03,  4.76544648e-02,\n           2.11831927e-02, -2.57394016e-02,  3.57929841e-02,\n          -5.46210110e-02, -8.11140314e-02,  6.16969913e-02,\n          -5.29541001e-02,  2.44519934e-02, -3.59662138e-02,\n           2.22078264e-02,  6.68109655e-02, -4.04674634e-02,\n           8.40559602e-04,  5.59461713e-02]],\n\n        [[-8.42168927e-04,  5.96341491e-03,  2.34083086e-03,\n           2.83593535e-02,  2.97983736e-03, -6.10663742e-03,\n           3.22834477e-02,  1.65871754e-02, -1.85287148e-02,\n          -8.00915137e-02, -3.40600312e-03,  1.31545961e-02,\n          -4.54472005e-03,  5.71999997e-02,  7.94627815e-02,\n          -5.39776944e-02, -2.04629004e-02,  7.50243664e-04,\n           6.39799088e-02,  6.71744347e-04,  7.67388642e-02,\n           1.91841125e-02, -3.56938615e-02,  5.73180616e-02,\n          -3.42374295e-02, -2.28424408e-02, -7.03058988e-02,\n          -2.50717439e-02, -2.07634717e-02,  1.63123012e-02,\n           1.65876001e-03, -6.97200000e-03]]],\n\n\n       [[[ 7.08167106e-02, -4.15653102e-02, -7.04499334e-02,\n           1.08586028e-02, -7.76553452e-02,  1.66892633e-02,\n          -2.55586244e-02, -5.44908047e-02,  2.38228068e-02,\n          -4.80652563e-02, -7.73183778e-02, -4.27093729e-02,\n           5.44033051e-02, -5.16761877e-02,  1.35839954e-02,\n          -2.96065956e-03,  4.14696634e-02, -7.54994825e-02,\n          -3.70977111e-02,  1.53204650e-02,  1.24688670e-02,\n           6.95328712e-02,  3.21613997e-03, -2.37783007e-02,\n           3.92514840e-02, -4.81264144e-02,  5.61483204e-02,\n          -2.73631476e-02, -7.92394876e-02,  2.09793821e-02,\n           3.23177502e-02, -4.39836457e-02]],\n\n        [[-7.25063086e-02, -8.43549147e-02,  4.01187390e-02,\n           4.84688580e-02, -4.24608290e-02,  1.86926350e-02,\n          -3.94862220e-02, -1.27617568e-02, -3.46452184e-02,\n           5.78139275e-02, -7.85863474e-02,  1.85164735e-02,\n           4.02261615e-02,  1.72825605e-02, -3.00131291e-02,\n           2.94023007e-03, -3.68990004e-04, -8.01117495e-02,\n          -1.11638308e-02,  2.67949179e-02,  9.90511477e-03,\n           8.38044137e-02, -1.85889006e-03,  2.51948982e-02,\n           1.25745684e-03,  4.01238650e-02,  1.64795369e-02,\n          -1.13520250e-02, -1.53782740e-02,  7.42655545e-02,\n           2.94291228e-03,  6.54212236e-02]],\n\n        [[ 4.07826751e-02,  4.22167629e-02,  7.05054402e-02,\n          -3.76420319e-02,  6.53587431e-02, -5.09557500e-02,\n          -7.03775063e-02, -2.10969150e-03, -1.13606676e-02,\n           2.80547142e-02, -7.23839477e-02,  1.72951818e-03,\n           3.58502343e-02,  5.92166185e-02, -7.02653974e-02,\n          -1.88585520e-02, -4.46446948e-02,  7.64969885e-02,\n           2.98979878e-03,  6.39982820e-02, -5.42797893e-03,\n          -7.13942647e-02,  5.48530817e-02,  2.97057256e-02,\n           3.33500206e-02, -2.58450955e-03,  2.59884447e-03,\n          -4.25752401e-02, -1.69291869e-02, -3.04224417e-02,\n          -8.34372714e-02, -1.80972219e-02]],\n\n        [[ 6.91497177e-02,  3.75580788e-02, -8.32781717e-02,\n          -2.14714631e-02,  2.32067183e-02,  5.96312135e-02,\n           6.72828108e-02,  3.17399576e-02,  7.50948638e-02,\n          -8.08411092e-02, -4.26320098e-02,  7.95608461e-02,\n           6.59239292e-03, -4.04824317e-03,  1.03071481e-02,\n           3.99650633e-02,  7.70720094e-02, -1.56719908e-02,\n          -4.49009016e-02, -1.95531473e-02,  4.65287119e-02,\n          -6.13850951e-02, -6.48038238e-02, -1.47439241e-02,\n           1.48249045e-02,  1.74490884e-02, -5.67066669e-02,\n          -1.75460428e-03, -4.39291969e-02, -4.25253659e-02,\n           4.03095186e-02,  1.55874491e-02]],\n\n        [[ 8.05509239e-02, -3.29339616e-02, -6.21445104e-02,\n          -3.46073397e-02, -2.49367580e-02,  8.49510282e-02,\n           7.81956017e-02,  1.72890723e-03, -1.14329532e-02,\n          -2.12855488e-02, -2.35341489e-02,  4.51733619e-02,\n          -3.50398272e-02, -4.03717905e-03,  3.42795625e-02,\n          -8.39426145e-02,  6.45890981e-02,  7.47614950e-02,\n          -1.47046596e-02,  3.59004959e-02,  2.65332982e-02,\n          -2.23290473e-02,  3.14059183e-02, -1.71203911e-02,\n           5.48159182e-02,  4.65227365e-02, -7.16410205e-02,\n           2.45064870e-02,  3.25034186e-02, -8.32620934e-02,\n          -4.80079986e-02,  5.76639026e-02]]],\n\n\n       [[[ 7.70103633e-02, -2.38308311e-03,  2.46418789e-02,\n          -8.20644498e-02, -4.60724160e-02, -6.45603240e-02,\n           7.53006339e-03,  3.62473875e-02,  5.33515960e-02,\n           5.53974509e-02,  2.13695318e-03, -5.81137724e-02,\n          -3.31136398e-02, -1.70647651e-02,  2.97719687e-02,\n          -3.31601165e-02,  6.23286366e-02,  3.70657444e-03,\n          -6.69322610e-02,  6.80418015e-02, -5.47790974e-02,\n          -8.23242962e-03,  4.75444794e-02,  7.67481625e-02,\n          -1.03140771e-02, -7.64542818e-03,  7.64319748e-02,\n          -8.05685967e-02,  1.45463273e-02,  6.21394217e-02,\n           5.94590008e-02,  8.42186958e-02]],\n\n        [[-2.15172991e-02, -8.09834152e-02,  6.51657432e-02,\n          -2.89698727e-02, -4.30320501e-02,  3.41561586e-02,\n          -2.63939612e-02, -5.95750436e-02, -1.70316547e-03,\n           4.45508361e-02,  6.54877722e-02,  1.26032680e-02,\n           2.56539658e-02,  1.94653496e-02, -6.94301799e-02,\n           7.18882680e-03,  5.47480285e-02,  3.41875926e-02,\n          -5.22099957e-02,  6.83116466e-02,  1.15721449e-02,\n           6.75344616e-02,  7.45189339e-02, -4.84893881e-02,\n          -8.37121904e-02,  7.83386528e-02,  7.17304498e-02,\n          -4.39944118e-03,  7.97368139e-02, -4.56971601e-02,\n           1.81042179e-02, -2.48248279e-02]],\n\n        [[-1.93358138e-02,  5.23116440e-02, -2.67364830e-02,\n           8.62997025e-03,  2.20735520e-02,  6.86911643e-02,\n           3.95361334e-02, -1.11933574e-02,  8.24351162e-02,\n           1.17632896e-02,  3.30266133e-02, -8.37104395e-02,\n          -6.92209154e-02,  9.80630517e-04,  4.35330719e-03,\n          -2.49019079e-02, -5.20139188e-03,  2.33110413e-02,\n          -7.84608573e-02, -2.24735662e-02, -7.22100288e-02,\n           2.33903602e-02,  5.70264906e-02, -4.79246154e-02,\n           6.37671798e-02, -2.93696895e-02,  4.20370698e-02,\n           7.51633644e-02,  3.63559872e-02,  7.28204846e-03,\n           1.46139786e-02, -6.13747239e-02]],\n\n        [[ 5.50170839e-02,  2.60977373e-02, -3.22272070e-02,\n           6.35971725e-02, -2.94441059e-02, -6.68592677e-02,\n          -1.59639269e-02,  1.69506595e-02, -6.99899495e-02,\n          -4.00409847e-03,  4.86414582e-02,  7.02627301e-02,\n           5.43139875e-02, -5.58452681e-02, -1.97886378e-02,\n          -2.25468054e-02,  5.79256117e-02,  4.24005687e-02,\n          -3.75262573e-02, -8.29783529e-02, -9.47709382e-03,\n           4.11262065e-02, -2.48581544e-02,  1.35859698e-02,\n           2.46383622e-02,  6.62097931e-02,  2.40958109e-02,\n           6.04675114e-02, -5.47211096e-02,  1.94521546e-02,\n          -6.63882494e-02, -8.16585347e-02]],\n\n        [[-7.21692964e-02, -4.82497290e-02, -1.13845393e-02,\n          -5.40195405e-03,  1.02636367e-02, -2.32595168e-02,\n           3.19817960e-02,  8.37199688e-02,  7.81929344e-02,\n           5.00818640e-02, -6.01271912e-02, -7.89984688e-02,\n           5.86514026e-02,  7.50228912e-02, -4.57147881e-02,\n           2.97245309e-02,  2.71621570e-02, -2.67836303e-02,\n          -1.97186917e-02, -3.08973640e-02, -3.15042660e-02,\n           3.43058258e-03, -1.57626346e-02, -1.01739466e-02,\n          -5.91270998e-02, -1.27623230e-02, -1.07187182e-02,\n           3.04223374e-02, -8.31662416e-02,  8.15691501e-02,\n          -1.16887316e-02, -7.88021758e-02]]],\n\n\n       [[[ 3.78468186e-02,  2.60055289e-02,  6.77331388e-02,\n          -2.69781724e-02,  7.56607056e-02, -6.53923079e-02,\n           2.70674303e-02, -9.51802731e-03,  4.94771302e-02,\n          -6.56722635e-02, -2.57047527e-02,  6.60489053e-02,\n           1.02866739e-02,  6.52538985e-02,  6.01759404e-02,\n           2.47062743e-03, -8.21836293e-04,  3.87634262e-02,\n           7.41065294e-03, -7.94431567e-03, -7.31641054e-02,\n           2.10043788e-03,  3.45099270e-02, -2.67503485e-02,\n           4.08622622e-02, -5.35255410e-02,  6.95426613e-02,\n          -7.54985511e-02,  3.64117175e-02,  1.60108134e-02,\n           1.08646601e-02, -4.57864814e-02]],\n\n        [[ 8.24451745e-02, -6.59886971e-02, -4.71873432e-02,\n           1.42157078e-02, -8.01478997e-02,  4.07901853e-02,\n           5.18690646e-02,  2.77418569e-02,  2.56453827e-02,\n           3.49805206e-02, -4.38251160e-02,  8.16472918e-02,\n          -2.08518803e-02, -8.46109390e-02, -6.42269701e-02,\n          -8.41284096e-02,  1.16627887e-02, -3.73849459e-02,\n          -6.10176250e-02, -7.14108199e-02, -7.79422373e-02,\n           8.70229304e-03, -3.99920903e-02,  1.27683654e-02,\n           7.32441992e-02, -8.22008178e-02, -2.71821767e-03,\n           2.80757770e-02, -6.50995374e-02,  6.37632608e-03,\n          -2.50077397e-02, -3.62034924e-02]],\n\n        [[ 4.63374853e-02,  3.70459482e-02,  4.47778106e-02,\n          -6.80370852e-02,  3.68895903e-02,  5.98122627e-02,\n          -6.04536459e-02,  8.46938789e-02, -1.89360380e-02,\n          -6.41082376e-02, -1.93697661e-02,  6.50997013e-02,\n          -3.50221172e-02, -4.50141318e-02,  4.45175171e-02,\n           6.06471747e-02, -3.38744558e-02,  2.58439109e-02,\n           3.99917662e-02,  6.70125484e-02, -4.55719121e-02,\n           6.94085807e-02, -7.83388019e-02, -5.83933219e-02,\n           5.31747639e-02,  1.69842690e-02,  4.67295945e-02,\n          -5.18412255e-02,  1.09351724e-02,  5.12499809e-02,\n           6.32988364e-02,  8.32819194e-03]],\n\n        [[ 2.70534232e-02,  2.80291513e-02,  1.71221048e-03,\n           3.55504155e-02,  7.85848498e-02,  2.45471075e-02,\n          -7.06288144e-02,  2.18944028e-02,  7.15783387e-02,\n           6.70951009e-02, -5.75722381e-02,  7.74262697e-02,\n          -3.21663506e-02,  2.85557434e-02, -5.27653322e-02,\n           3.15729678e-02, -6.73605055e-02,  4.71258014e-02,\n          -4.70406562e-03, -9.84051824e-03,  7.10418671e-02,\n          -4.95103188e-02,  1.77704617e-02,  1.58598423e-02,\n          -2.16057822e-02, -6.13741130e-02,  7.65690655e-02,\n          -8.25137645e-03, -6.36689365e-04, -1.99301243e-02,\n          -2.60162652e-02,  2.73930728e-02]],\n\n        [[ 5.82411587e-02,  2.84197628e-02,  8.34772438e-02,\n           6.68082386e-03, -1.87359005e-03,  2.99061835e-02,\n           7.09632635e-02,  5.64216077e-02,  4.17516232e-02,\n           8.25207978e-02, -4.87305112e-02, -1.94430649e-02,\n          -1.22744069e-02,  6.71185255e-02,  7.48435855e-02,\n          -1.15827844e-02,  5.71184456e-02,  2.43371353e-02,\n           5.16017973e-02,  4.44789231e-02,  5.22666574e-02,\n          -6.94158077e-02, -2.76609138e-02,  5.45022041e-02,\n           7.22679496e-02,  2.92573571e-02, -3.59787792e-02,\n          -2.49298252e-02, -6.37726635e-02,  7.47311115e-03,\n           7.76798129e-02,  2.46423632e-02]]]], dtype=float32)>: ['conv2d_114/kernel']\n    <tf.Variable 'conv2d_114/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['conv2d_114/bias']\n    <tf.Variable 'batch_normalization_129/gamma:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_129/gamma']\n    <tf.Variable 'batch_normalization_129/beta:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_129/beta']\n    <tf.Variable 'batch_normalization_129/moving_mean:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_129/moving_mean']\n    <tf.Variable 'batch_normalization_129/moving_variance:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_129/moving_variance']\n    <tf.Variable 'conv2d_115/kernel:0' shape=(3, 3, 32, 32) dtype=float32, numpy=\narray([[[[ 0.08894353, -0.05977122,  0.05894132, ..., -0.04162906,\n           0.02519324, -0.09510606],\n         [ 0.03003617,  0.03976461,  0.05913787, ...,  0.04029824,\n          -0.00649183,  0.00264079],\n         [-0.00435548, -0.06751756, -0.10098987, ...,  0.06662126,\n          -0.08595265,  0.062951  ],\n         ...,\n         [-0.08272442,  0.05674174,  0.04043259, ...,  0.08818418,\n           0.08564639, -0.0828737 ],\n         [-0.09780145,  0.00107975,  0.03474534, ...,  0.05599388,\n           0.0935244 ,  0.07212874],\n         [ 0.07871538, -0.02359098,  0.09242372, ...,  0.05296732,\n           0.05820541,  0.02591023]],\n\n        [[-0.04237591,  0.10151696,  0.05251798, ..., -0.02696823,\n          -0.00659166,  0.06681097],\n         [-0.06719928, -0.05489751, -0.01801632, ..., -0.04680521,\n           0.09253623,  0.06679554],\n         [ 0.06629281,  0.09349979,  0.06026611, ...,  0.08937   ,\n          -0.05042158,  0.00784368],\n         ...,\n         [ 0.08094649, -0.02040904,  0.04319829, ..., -0.05099342,\n           0.07327187,  0.03586023],\n         [-0.01352485, -0.06036359, -0.09180471, ..., -0.00549743,\n          -0.02357962, -0.10205168],\n         [-0.07485071, -0.08910556,  0.00868326, ...,  0.00876678,\n          -0.10099641, -0.04902995]],\n\n        [[ 0.03439131,  0.04943204, -0.0916671 , ..., -0.00508028,\n           0.0555833 , -0.10143805],\n         [ 0.09338669,  0.10032207,  0.0538581 , ..., -0.00236005,\n           0.00637615,  0.0784574 ],\n         [ 0.04138272, -0.04026536, -0.05924457, ...,  0.0671355 ,\n           0.09607543, -0.03936634],\n         ...,\n         [-0.00871604, -0.03516168,  0.04433656, ..., -0.05127683,\n          -0.02237319, -0.075156  ],\n         [-0.09733602, -0.03928996, -0.09156368, ...,  0.01875317,\n          -0.03495908, -0.05530023],\n         [-0.04071162, -0.01651368,  0.02589984, ..., -0.03172555,\n           0.0981721 , -0.00238522]]],\n\n\n       [[[ 0.0620397 , -0.01395828,  0.04724044, ...,  0.00929736,\n          -0.04187829,  0.07943258],\n         [ 0.09680519, -0.03939717, -0.0457579 , ..., -0.02058422,\n          -0.03641162, -0.06849881],\n         [ 0.07510222,  0.06301501, -0.01792016, ..., -0.07129081,\n          -0.06565954, -0.00864362],\n         ...,\n         [ 0.05879474, -0.07695948, -0.03712591, ..., -0.07289186,\n          -0.03883222,  0.03090015],\n         [-0.09596459,  0.04458947,  0.07616664, ..., -0.07194636,\n          -0.07698016, -0.08578504],\n         [-0.0346738 , -0.09395652, -0.09684777, ...,  0.07793161,\n           0.01901412,  0.03697993]],\n\n        [[-0.07912328,  0.06141171, -0.09213284, ...,  0.06304573,\n           0.01559288, -0.02195154],\n         [ 0.0148547 ,  0.08717996, -0.04478246, ...,  0.08632775,\n           0.10205236, -0.07149363],\n         [-0.04560793,  0.03222919,  0.09684677, ..., -0.01836649,\n          -0.09414296, -0.02157916],\n         ...,\n         [ 0.06407459,  0.05868796, -0.04207882, ..., -0.08945329,\n          -0.02850287,  0.08666797],\n         [-0.0564331 ,  0.02840702,  0.05653425, ..., -0.06775163,\n          -0.09597824,  0.07533146],\n         [ 0.03162269,  0.05734962, -0.04673111, ..., -0.02168647,\n          -0.09085219,  0.03113756]],\n\n        [[ 0.03547588, -0.02106884,  0.03564803, ..., -0.03206116,\n          -0.02762562, -0.06682092],\n         [ 0.05365343, -0.08919559, -0.06454669, ...,  0.08827245,\n          -0.04603099,  0.09574811],\n         [-0.06542459, -0.04988228,  0.05780156, ..., -0.04713773,\n           0.07599083, -0.04758558],\n         ...,\n         [ 0.09420523, -0.03136744,  0.02756457, ...,  0.0258453 ,\n          -0.09475578,  0.04846892],\n         [ 0.03974558,  0.00399264, -0.03045838, ..., -0.06767476,\n          -0.03899238,  0.09514453],\n         [-0.08320564, -0.05849544,  0.08917265, ...,  0.04358241,\n          -0.06216218, -0.02731756]]],\n\n\n       [[[ 0.05137509, -0.01093893, -0.01660357, ...,  0.08871579,\n          -0.07828309,  0.06777547],\n         [ 0.03767197, -0.06856626, -0.01336688, ...,  0.01829906,\n           0.07840005,  0.02403153],\n         [-0.01591724, -0.00534578,  0.00113757, ...,  0.04423308,\n          -0.00991281, -0.06822645],\n         ...,\n         [ 0.07546867,  0.04941963,  0.02619034, ...,  0.10102426,\n           0.06489037,  0.03450869],\n         [ 0.0395615 , -0.04127698, -0.09556214, ..., -0.0048686 ,\n           0.02355589,  0.0869129 ],\n         [-0.03414661, -0.04860034,  0.06294554, ..., -0.090893  ,\n           0.05675966, -0.01181561]],\n\n        [[-0.06262171,  0.01091676,  0.0903264 , ..., -0.04323716,\n           0.00634228,  0.03258082],\n         [-0.06421553,  0.10183108, -0.08845873, ...,  0.07690609,\n           0.07459472, -0.00943173],\n         [-0.02010056, -0.078018  , -0.09281311, ...,  0.0975914 ,\n          -0.0802223 , -0.01641832],\n         ...,\n         [-0.08029749, -0.05407443,  0.05013904, ..., -0.05646541,\n          -0.04274823,  0.01729671],\n         [-0.03296327, -0.09287234, -0.0129387 , ..., -0.04301468,\n          -0.07404072, -0.01288386],\n         [-0.0986732 ,  0.00915796, -0.03652263, ..., -0.05706514,\n          -0.0150627 , -0.08883035]],\n\n        [[ 0.03350657,  0.09461167,  0.01703417, ...,  0.06641054,\n          -0.03846505, -0.08721709],\n         [-0.01377843, -0.01191448,  0.07536598, ...,  0.09785073,\n          -0.01292726,  0.08728831],\n         [ 0.09276715, -0.01423037, -0.06214957, ...,  0.03962807,\n           0.10047069, -0.04038662],\n         ...,\n         [-0.08180322, -0.02183474,  0.08713834, ...,  0.08776809,\n           0.00130038,  0.04784954],\n         [-0.03813103, -0.0397272 ,  0.02303384, ...,  0.0239353 ,\n          -0.0607739 ,  0.09779748],\n         [-0.01564303, -0.06503414,  0.01879495, ...,  0.00814204,\n          -0.04783155, -0.08780671]]]], dtype=float32)>: ['conv2d_115/kernel']\n    <tf.Variable 'conv2d_115/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['conv2d_115/bias']\n    <tf.Variable 'batch_normalization_130/gamma:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_130/gamma']\n    <tf.Variable 'batch_normalization_130/beta:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_130/beta']\n    <tf.Variable 'batch_normalization_130/moving_mean:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_130/moving_mean']\n    <tf.Variable 'batch_normalization_130/moving_variance:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_130/moving_variance']\n    <tf.Variable 'conv2d_116/kernel:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\narray([[[[ 0.06786553, -0.0261334 ,  0.05468185, ...,  0.05123378,\n           0.04701877, -0.04092249],\n         [-0.00363847, -0.01574651, -0.04981294, ..., -0.04060807,\n          -0.07870933, -0.01496635],\n         [ 0.06977115, -0.06160953,  0.06276996, ..., -0.01680875,\n           0.0556236 ,  0.00582371],\n         ...,\n         [ 0.053202  , -0.04889594,  0.04198524, ...,  0.01900645,\n          -0.0331565 ,  0.01057426],\n         [-0.0501347 , -0.03942879,  0.01960655, ...,  0.01956026,\n          -0.05039076, -0.04256004],\n         [-0.06611538, -0.06159077, -0.07272055, ..., -0.023371  ,\n          -0.0026655 , -0.00966045]],\n\n        [[ 0.06185357,  0.08331808, -0.00642395, ...,  0.06653073,\n          -0.06299537,  0.00163712],\n         [ 0.05239757, -0.0769107 ,  0.06467684, ..., -0.07825466,\n          -0.03137428,  0.00842508],\n         [ 0.07060862, -0.00917542, -0.00881752, ..., -0.04073662,\n          -0.01588174, -0.01227534],\n         ...,\n         [-0.04347279,  0.05009105,  0.0205672 , ...,  0.06649119,\n           0.0721819 ,  0.00302943],\n         [-0.05579249,  0.06492988,  0.00272594, ...,  0.00871736,\n          -0.01420721,  0.04369409],\n         [ 0.05928604, -0.03155796, -0.0394375 , ..., -0.0519192 ,\n           0.0780837 ,  0.0127368 ]],\n\n        [[ 0.02232895, -0.03892915, -0.07620295, ..., -0.07362779,\n           0.02034465,  0.04515404],\n         [ 0.04273409, -0.00619683,  0.02169609, ...,  0.06995317,\n           0.02664389,  0.04807133],\n         [-0.04848013,  0.05660107,  0.0179223 , ..., -0.01286534,\n          -0.043562  ,  0.00670568],\n         ...,\n         [ 0.0466852 ,  0.04046664, -0.03091624, ..., -0.03739556,\n           0.04801627,  0.005681  ],\n         [-0.03502915, -0.02775478,  0.06107005, ..., -0.02930908,\n           0.04779597,  0.05749723],\n         [ 0.04604412,  0.07484207,  0.04582614, ...,  0.01780865,\n          -0.08090061, -0.07922006]]],\n\n\n       [[[ 0.05491076, -0.00425478, -0.02198982, ...,  0.0025504 ,\n           0.01308123, -0.02995614],\n         [-0.03144783, -0.05735376, -0.03113335, ...,  0.01946803,\n          -0.05290896, -0.02477866],\n         [-0.0105044 ,  0.0075049 , -0.01178054, ..., -0.00464308,\n           0.01379702, -0.03651337],\n         ...,\n         [-0.03535529,  0.03391626, -0.05554207, ..., -0.0733374 ,\n           0.0512488 , -0.02560636],\n         [ 0.07440766,  0.05216945, -0.07307305, ...,  0.01966017,\n          -0.06616855, -0.00081893],\n         [-0.03514791,  0.04141579,  0.04481114, ...,  0.06427529,\n          -0.05128245, -0.04337732]],\n\n        [[ 0.01960317,  0.08110889, -0.05065569, ...,  0.07411543,\n          -0.02563556,  0.03024834],\n         [ 0.077209  , -0.04263089, -0.06207162, ...,  0.02550242,\n           0.07908883, -0.02494661],\n         [-0.0137411 ,  0.02905273,  0.020271  , ...,  0.0148167 ,\n          -0.0146755 , -0.05974553],\n         ...,\n         [ 0.07827396,  0.02542019, -0.04105077, ...,  0.07301182,\n           0.08015092,  0.07740734],\n         [ 0.03797013,  0.01828101,  0.06534401, ..., -0.01939579,\n           0.04883713, -0.04024935],\n         [-0.01840176,  0.04549287, -0.05745634, ..., -0.01360223,\n          -0.06846356,  0.07146534]],\n\n        [[-0.08174227, -0.04319054,  0.00514207, ..., -0.01205786,\n          -0.03296026,  0.04976875],\n         [ 0.05354258, -0.00556201,  0.0582151 , ..., -0.04334138,\n           0.06577963,  0.02623764],\n         [-0.04761519, -0.02017347, -0.00843507, ..., -0.03588595,\n           0.01288992,  0.06692273],\n         ...,\n         [ 0.00411558,  0.04135039,  0.03604009, ..., -0.01093256,\n           0.01114064,  0.00543308],\n         [-0.03374019, -0.024954  ,  0.02764913, ...,  0.06904659,\n           0.04201163,  0.01444662],\n         [-0.06684017, -0.05146621,  0.06558526, ...,  0.07234574,\n           0.07118727, -0.05247933]]],\n\n\n       [[[ 0.02077695, -0.00187305,  0.03575812, ...,  0.02982893,\n           0.05027277,  0.04850861],\n         [ 0.03044951,  0.06913639, -0.03477621, ..., -0.07878844,\n           0.00798943, -0.04601198],\n         [ 0.06535903,  0.00629401, -0.00410497, ..., -0.05560794,\n           0.0392978 , -0.00123346],\n         ...,\n         [-0.04378315,  0.06720487,  0.01940614, ..., -0.04239967,\n          -0.05567314,  0.03575023],\n         [ 0.04297205, -0.0456691 , -0.02656518, ...,  0.03513962,\n          -0.00659426,  0.05752993],\n         [-0.02449781, -0.07707467, -0.07209063, ...,  0.01041081,\n          -0.04121944,  0.01540587]],\n\n        [[ 0.00215662,  0.0820801 ,  0.01990247, ..., -0.06442132,\n          -0.00513661, -0.04317605],\n         [ 0.05494503, -0.02374178, -0.04593595, ...,  0.02747013,\n           0.02792851, -0.06234119],\n         [-0.02065265, -0.02749845, -0.07035393, ...,  0.03303631,\n          -0.00575292, -0.041046  ],\n         ...,\n         [ 0.02054892,  0.02918879,  0.04967926, ..., -0.00331519,\n           0.032446  , -0.00586434],\n         [ 0.01392857,  0.04845939, -0.06637548, ...,  0.00743705,\n          -0.01400916,  0.08319924],\n         [-0.02172425, -0.01518768,  0.01606623, ..., -0.01281329,\n          -0.05880431,  0.0314281 ]],\n\n        [[ 0.05677726,  0.01288529, -0.08178856, ...,  0.0045661 ,\n          -0.0011861 , -0.04291843],\n         [-0.05863401, -0.07951623, -0.07241204, ...,  0.03616643,\n           0.03830548, -0.07005432],\n         [-0.03403916,  0.0271456 ,  0.01232588, ..., -0.02565426,\n          -0.02148436, -0.07235152],\n         ...,\n         [-0.04798188, -0.01486143,  0.06022414, ...,  0.05292798,\n          -0.0195879 , -0.05546286],\n         [ 0.0536802 , -0.02341622, -0.05619989, ..., -0.02107247,\n           0.01460256, -0.01807783],\n         [ 0.02481482,  0.01606518, -0.05635709, ..., -0.0504626 ,\n          -0.05637823, -0.05407457]]]], dtype=float32)>: ['conv2d_116/kernel']\n    <tf.Variable 'conv2d_116/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_116/bias']\n    <tf.Variable 'batch_normalization_131/gamma:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_131/gamma']\n    <tf.Variable 'batch_normalization_131/beta:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_131/beta']\n    <tf.Variable 'batch_normalization_131/moving_mean:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_131/moving_mean']\n    <tf.Variable 'batch_normalization_131/moving_variance:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_131/moving_variance']\n    <tf.Variable 'conv2d_117/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=\narray([[[[ 0.04438304, -0.04285087,  0.01372197, ..., -0.00011134,\n          -0.03350283,  0.033383  ],\n         [-0.05145435,  0.0066842 ,  0.0524209 , ...,  0.0653877 ,\n          -0.02055708,  0.04530247],\n         [ 0.03368892,  0.02164079, -0.07096566, ..., -0.0069989 ,\n          -0.04764551,  0.06185031],\n         ...,\n         [-0.04843613, -0.01151731, -0.03517066, ...,  0.06599356,\n          -0.05435172, -0.00555785],\n         [-0.0248896 , -0.04914992, -0.04809322, ..., -0.06303603,\n           0.03960167,  0.02678834],\n         [-0.01191647, -0.06993486,  0.04567583, ..., -0.02403647,\n           0.00416996,  0.02237256]],\n\n        [[-0.05666338, -0.06741683,  0.01532263, ...,  0.02284519,\n          -0.06029843,  0.06942776],\n         [-0.06540984,  0.05055628,  0.06774756, ..., -0.06818701,\n          -0.04264463,  0.03898373],\n         [ 0.05039816, -0.06217661, -0.0492875 , ..., -0.04672299,\n           0.04685744, -0.01774649],\n         ...,\n         [ 0.06399538, -0.03546615, -0.02212004, ..., -0.02660194,\n           0.05024142,  0.06244567],\n         [ 0.05032002, -0.03508443, -0.01643254, ..., -0.00415868,\n           0.06462857,  0.00122332],\n         [ 0.02001463,  0.01953696, -0.03275201, ...,  0.03784043,\n           0.03640389,  0.03026084]],\n\n        [[-0.04184863, -0.0085455 , -0.02683187, ...,  0.05295728,\n           0.06530787,  0.07112913],\n         [ 0.03992489, -0.02688261, -0.06522462, ..., -0.04686223,\n           0.04436062,  0.06112383],\n         [-0.03357723, -0.00983307, -0.05401913, ..., -0.03685551,\n          -0.00376391, -0.01079582],\n         ...,\n         [ 0.00457259, -0.07094861,  0.0460915 , ..., -0.05403766,\n           0.03437745, -0.06845114],\n         [-0.04561963,  0.01354801, -0.06585649, ...,  0.01189281,\n           0.02888505, -0.02551439],\n         [ 0.04832169, -0.04616661, -0.06813759, ..., -0.01463365,\n           0.05538391,  0.02185833]]],\n\n\n       [[[-0.04970164, -0.01272995,  0.04075366, ...,  0.03772872,\n          -0.0674525 , -0.06899706],\n         [-0.02783239, -0.00629096,  0.01843622, ...,  0.0132632 ,\n           0.03179472,  0.02738975],\n         [ 0.0435734 , -0.04655456, -0.05144501, ...,  0.03502637,\n          -0.03955234,  0.04637276],\n         ...,\n         [ 0.0121603 ,  0.03119673, -0.03320348, ...,  0.0071391 ,\n          -0.0612465 , -0.00995996],\n         [-0.07110415,  0.05117569, -0.03385988, ..., -0.05418261,\n          -0.0635227 ,  0.05754654],\n         [-0.02510001, -0.00163276, -0.01566654, ..., -0.03747276,\n          -0.00690898,  0.00663006]],\n\n        [[ 0.06959999, -0.04133851,  0.05813232, ..., -0.06369941,\n          -0.0332462 ,  0.00445484],\n         [ 0.02912505, -0.06469519,  0.04489003, ..., -0.00385603,\n           0.02932602, -0.00194353],\n         [ 0.05678529,  0.07034063,  0.0484549 , ..., -0.03730074,\n          -0.05990511, -0.03386389],\n         ...,\n         [ 0.06124784, -0.00156062,  0.05893785, ..., -0.03772413,\n          -0.0540878 , -0.02331681],\n         [ 0.06264238,  0.02784982,  0.0576838 , ..., -0.04768804,\n           0.04428057,  0.04030138],\n         [-0.01988168,  0.0681812 , -0.06251813, ..., -0.0713319 ,\n           0.07074553,  0.03368422]],\n\n        [[ 0.07071967,  0.05504942,  0.02465089, ...,  0.06811538,\n           0.03719527,  0.03165621],\n         [-0.04879937,  0.06996217,  0.06860951, ...,  0.03760716,\n           0.05242384,  0.00094993],\n         [ 0.05609709, -0.06910016, -0.00144058, ..., -0.05156258,\n          -0.03266622,  0.00507005],\n         ...,\n         [-0.03941873, -0.01882354,  0.06653954, ..., -0.02171435,\n          -0.03149492,  0.04557445],\n         [-0.02137103,  0.00311795, -0.04449124, ..., -0.05712911,\n          -0.01872971,  0.00584043],\n         [-0.05034996,  0.02177434, -0.0224377 , ...,  0.01662668,\n          -0.02704531, -0.04708452]]],\n\n\n       [[[ 0.05569963, -0.03258986, -0.00135416, ...,  0.04769374,\n           0.06976673,  0.01427959],\n         [ 0.0194154 , -0.05084968, -0.02335573, ..., -0.03606197,\n           0.02512049,  0.03126741],\n         [ 0.04725768,  0.0075729 , -0.01658427, ..., -0.02292686,\n           0.0249723 , -0.04305361],\n         ...,\n         [ 0.02627002, -0.01923196,  0.04843508, ...,  0.04033419,\n          -0.01041249, -0.04651652],\n         [-0.0032336 , -0.05547963,  0.04287176, ..., -0.04120681,\n           0.06320404,  0.06568711],\n         [ 0.01238833,  0.06561546,  0.04218252, ..., -0.05353834,\n          -0.07088326,  0.06038113]],\n\n        [[ 0.0588329 , -0.06585014,  0.00516375, ...,  0.0631793 ,\n          -0.04019496, -0.00482945],\n         [-0.06247793, -0.05692145,  0.05196715, ..., -0.05612189,\n          -0.01730888,  0.01347722],\n         [ 0.03445052, -0.02037941,  0.05801201, ..., -0.00157129,\n           0.01940921, -0.02310072],\n         ...,\n         [ 0.02310533,  0.03313166,  0.03858825, ...,  0.05428907,\n          -0.01432565,  0.01131889],\n         [-0.06108302,  0.06623052, -0.05047789, ..., -0.03463778,\n          -0.02358206,  0.03197179],\n         [ 0.06087202, -0.06729334, -0.03871019, ...,  0.01125055,\n          -0.03692764,  0.02934371]],\n\n        [[ 0.02823823, -0.05539232, -0.03197594, ..., -0.0659197 ,\n          -0.00187466,  0.03192572],\n         [ 0.01177758,  0.05835094, -0.04251462, ..., -0.00739201,\n          -0.00145199, -0.06548118],\n         [ 0.0012126 ,  0.0084261 ,  0.01720739, ..., -0.00425037,\n          -0.04573406,  0.05145468],\n         ...,\n         [-0.03969051, -0.0525812 , -0.02062179, ..., -0.05824813,\n           0.06520851,  0.03068282],\n         [-0.03640494,  0.01628947, -0.07133237, ..., -0.02125225,\n          -0.06235721, -0.00847837],\n         [ 0.02021034,  0.07002898, -0.02424689, ..., -0.04233089,\n          -0.00221475,  0.06101018]]]], dtype=float32)>: ['conv2d_117/kernel']\n    <tf.Variable 'conv2d_117/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_117/bias']\n    <tf.Variable 'batch_normalization_132/gamma:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_132/gamma']\n    <tf.Variable 'batch_normalization_132/beta:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_132/beta']\n    <tf.Variable 'batch_normalization_132/moving_mean:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_132/moving_mean']\n    <tf.Variable 'batch_normalization_132/moving_variance:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_132/moving_variance']\n    <tf.Variable 'conv2d_118/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=\narray([[[[ 0.03877113,  0.01833312,  0.01893025, ..., -0.05776759,\n          -0.06151442, -0.03601512],\n         [-0.05116709,  0.05850439,  0.01984343, ..., -0.01334621,\n           0.02125169, -0.0350797 ],\n         [-0.00070491, -0.03257178,  0.06816809, ..., -0.03471268,\n          -0.04834292,  0.05908708],\n         ...,\n         [ 0.04942653,  0.06266218, -0.0021069 , ..., -0.00438359,\n          -0.06648469,  0.04549726],\n         [-0.03526551,  0.03443979, -0.04499265, ..., -0.06243909,\n          -0.05706972,  0.06691751],\n         [-0.06174187,  0.03836393, -0.03227536, ..., -0.00280215,\n           0.02770991,  0.05923393]],\n\n        [[ 0.04542252,  0.00117584, -0.01835831, ..., -0.05846206,\n           0.01748011,  0.00476333],\n         [ 0.02568193,  0.07112138,  0.01883388, ..., -0.01682904,\n           0.07093517,  0.02567424],\n         [ 0.0214892 ,  0.06370644,  0.06460989, ..., -0.06226235,\n           0.06367651, -0.04267108],\n         ...,\n         [-0.01231925, -0.03878116, -0.01435659, ..., -0.04112145,\n           0.02065908, -0.05534385],\n         [-0.01011064, -0.06449308,  0.04553054, ..., -0.06459568,\n          -0.00621687, -0.04857603],\n         [-0.0565642 , -0.06821758, -0.03413916, ...,  0.05004583,\n           0.03125704, -0.05878351]],\n\n        [[-0.04761552, -0.06600253, -0.0518624 , ..., -0.01935761,\n          -0.0241734 , -0.02307914],\n         [-0.05798317,  0.0470167 , -0.05382694, ..., -0.02734956,\n           0.03134477,  0.02986907],\n         [ 0.04247741, -0.0155019 , -0.03319821, ..., -0.00305159,\n           0.06144111, -0.05329078],\n         ...,\n         [ 0.04071425, -0.06901441,  0.05070599, ..., -0.06647912,\n          -0.02296019,  0.04623763],\n         [-0.0565888 , -0.05815906,  0.00165781, ..., -0.06593505,\n           0.00515372, -0.06898157],\n         [ 0.01733115,  0.07154359, -0.03129489, ..., -0.02506095,\n           0.02996183, -0.04260403]]],\n\n\n       [[[-0.04516261,  0.0463337 , -0.04536558, ...,  0.06501704,\n           0.00568268, -0.05899135],\n         [ 0.04214638,  0.00988057,  0.00089636, ..., -0.07072169,\n          -0.04104153,  0.02192716],\n         [-0.01770686, -0.05478507, -0.05104627, ...,  0.05898683,\n          -0.06822073, -0.03223513],\n         ...,\n         [ 0.06841086,  0.03417531,  0.0299482 , ..., -0.01076939,\n           0.03749533,  0.00398302],\n         [-0.03629133,  0.02187862, -0.0258222 , ...,  0.05863181,\n          -0.05516243, -0.01502142],\n         [-0.02733949, -0.05090197, -0.01433316, ...,  0.05674313,\n          -0.04189938,  0.01198992]],\n\n        [[ 0.03188954, -0.07039972,  0.05304247, ..., -0.0655705 ,\n          -0.0065212 , -0.06953745],\n         [-0.04539733, -0.00151605, -0.0518778 , ...,  0.07069567,\n           0.02648086,  0.0553602 ],\n         [ 0.0621936 ,  0.03769825,  0.03377232, ...,  0.05826665,\n          -0.06291612, -0.00311076],\n         ...,\n         [-0.00730772,  0.04134034,  0.00104246, ..., -0.02777834,\n           0.04989239,  0.04253239],\n         [-0.03053559, -0.0093063 ,  0.06875573, ...,  0.06139988,\n           0.0020249 , -0.05570515],\n         [ 0.06723271, -0.03812815,  0.00459572, ..., -0.05536314,\n          -0.03794863, -0.02327976]],\n\n        [[-0.04645947,  0.03012734,  0.0161233 , ..., -0.05706478,\n          -0.04067849, -0.01412322],\n         [-0.04957503, -0.02838425, -0.01845252, ..., -0.05008958,\n          -0.06112057, -0.00776485],\n         [-0.00217076, -0.03719087,  0.04201579, ...,  0.06649381,\n          -0.05802302, -0.04026086],\n         ...,\n         [ 0.03809988, -0.01834358, -0.00597712, ..., -0.03900846,\n           0.05055071,  0.03343792],\n         [ 0.00275002,  0.00485256,  0.05018485, ..., -0.06025784,\n           0.04186558,  0.03521121],\n         [ 0.0118692 ,  0.02633362,  0.05148832, ..., -0.06171424,\n          -0.06679129, -0.06939138]]],\n\n\n       [[[-0.0040705 , -0.06354765,  0.01615597, ...,  0.06927787,\n           0.05164745, -0.01483686],\n         [ 0.07025413,  0.05731997, -0.00689374, ...,  0.02061816,\n          -0.07158032, -0.058349  ],\n         [ 0.05394121, -0.0252414 , -0.01950267, ..., -0.04081823,\n          -0.01345436,  0.018637  ],\n         ...,\n         [-0.06631189, -0.00459479,  0.0194456 , ...,  0.00292176,\n           0.01898488,  0.01096507],\n         [ 0.05909352,  0.05787852, -0.04479546, ..., -0.07010835,\n          -0.00232496,  0.04265188],\n         [-0.01702597,  0.02992214,  0.01510631, ...,  0.04920357,\n          -0.01130041, -0.06844547]],\n\n        [[-0.06974315, -0.05750525,  0.05637971, ...,  0.04629767,\n          -0.01945687,  0.06011067],\n         [-0.00213712,  0.00575739, -0.06786154, ...,  0.01156081,\n          -0.00796626,  0.03657601],\n         [ 0.00955804,  0.00554314,  0.03028002, ..., -0.06273021,\n           0.03617402, -0.00306208],\n         ...,\n         [-0.00913791,  0.05858643, -0.0342128 , ..., -0.02685412,\n          -0.00211862,  0.05895548],\n         [-0.04518333, -0.02263317, -0.061915  , ..., -0.03589646,\n          -0.04154506, -0.04066733],\n         [-0.05078237,  0.03219026, -0.01999066, ...,  0.00875963,\n          -0.04741736, -0.05800588]],\n\n        [[-0.03001233, -0.01149807, -0.01185129, ..., -0.06561344,\n           0.00220028, -0.0018803 ],\n         [ 0.04597792,  0.00784519, -0.01591411, ..., -0.00528041,\n           0.04508949,  0.01272945],\n         [-0.03790894,  0.03259083, -0.0264221 , ..., -0.05438966,\n           0.00305047,  0.00836204],\n         ...,\n         [-0.03962624,  0.05532111,  0.00554682, ...,  0.01193276,\n           0.03864805,  0.0569544 ],\n         [ 0.03286623,  0.03293015,  0.04660929, ...,  0.03876019,\n          -0.04701607, -0.04473887],\n         [ 0.03670609, -0.03495495,  0.02037287, ..., -0.05017339,\n          -0.0045495 , -0.00400575]]]], dtype=float32)>: ['conv2d_118/kernel']\n    <tf.Variable 'conv2d_118/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_118/bias']\n    <tf.Variable 'batch_normalization_133/gamma:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_133/gamma']\n    <tf.Variable 'batch_normalization_133/beta:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_133/beta']\n    <tf.Variable 'batch_normalization_133/moving_mean:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_133/moving_mean']\n    <tf.Variable 'batch_normalization_133/moving_variance:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_133/moving_variance']\n    <tf.Variable 'conv2d_119/kernel:0' shape=(3, 3, 64, 128) dtype=float32, numpy=\narray([[[[ 5.92639670e-03,  3.67051177e-02, -3.15407291e-03, ...,\n          -1.11163780e-03,  4.50912006e-02, -9.13154334e-04],\n         [-4.50183973e-02,  3.69103439e-02, -4.35523614e-02, ...,\n           2.20872276e-02, -8.50735605e-03,  6.83987513e-03],\n         [ 2.07550824e-03,  2.06881873e-02, -7.99454749e-04, ...,\n           1.69167928e-02,  4.24238667e-03,  3.15575786e-02],\n         ...,\n         [-4.20955569e-03,  1.03060119e-02, -3.34499329e-02, ...,\n           3.75226401e-02,  2.95442343e-03,  9.13659111e-03],\n         [ 5.06296642e-02,  6.05051592e-03, -1.44848451e-02, ...,\n          -4.75230999e-02,  7.66558573e-03, -4.68068868e-02],\n         [-2.62558460e-05,  2.77219601e-02,  5.84110953e-02, ...,\n           5.63734062e-02,  5.09757437e-02,  4.89783473e-02]],\n\n        [[-1.58014558e-02, -5.27883396e-02, -4.35903780e-02, ...,\n           1.11281760e-02,  5.47892340e-02,  1.55311115e-02],\n         [ 6.07189909e-03, -1.85826868e-02,  2.27298550e-02, ...,\n          -3.09677627e-02, -4.90207039e-02, -9.70738754e-03],\n         [-1.71526857e-02,  2.70762667e-03,  4.96365540e-02, ...,\n           3.61494906e-02,  1.86730362e-02, -5.29034138e-02],\n         ...,\n         [ 2.29848139e-02,  3.51279639e-02,  3.34672369e-02, ...,\n           1.85861103e-02, -2.05486268e-02, -5.01186401e-03],\n         [-5.45427725e-02,  4.56480049e-02,  4.66168486e-02, ...,\n           3.24751027e-02,  1.22028776e-02, -8.99219885e-03],\n         [ 1.51127614e-02,  5.80065139e-02, -3.74097116e-02, ...,\n          -2.66156644e-02, -2.65194289e-02,  6.47733733e-03]],\n\n        [[ 5.05826510e-02,  1.32703967e-02,  4.88886870e-02, ...,\n           1.63400136e-02, -2.91801747e-02,  1.37164108e-02],\n         [ 4.86677103e-02,  2.19557434e-04,  5.58342896e-02, ...,\n          -2.69679688e-02,  4.29415964e-02,  4.83439490e-03],\n         [ 2.55890377e-02, -5.10364920e-02,  2.58725509e-03, ...,\n           1.68920495e-02,  2.78913118e-02, -5.74136302e-02],\n         ...,\n         [ 2.68697366e-03,  3.38242762e-02,  1.18476935e-02, ...,\n           5.05354889e-02,  9.12024453e-03,  3.82690690e-02],\n         [-3.41674685e-02, -2.05314308e-02,  1.84942745e-02, ...,\n          -2.25872844e-02, -5.58495335e-02,  3.38141806e-02],\n         [ 7.25823268e-03, -2.69127153e-02, -8.69541615e-03, ...,\n           2.81900205e-02, -2.23457813e-02,  1.59018375e-02]]],\n\n\n       [[[ 2.32915953e-03,  2.94929035e-02,  3.72573622e-02, ...,\n           5.55808060e-02, -2.74109095e-03, -5.54106012e-02],\n         [ 4.68080081e-02,  2.01425292e-02,  4.59944643e-02, ...,\n          -5.16686663e-02, -3.53650004e-02,  3.48044597e-02],\n         [ 4.30017300e-02, -4.46912386e-02,  9.46910307e-03, ...,\n          -5.29691465e-02, -2.65936516e-02, -5.22887856e-02],\n         ...,\n         [ 4.96440418e-02,  5.63724674e-02, -1.36712417e-02, ...,\n           1.03393681e-02, -6.24717399e-03, -1.06401816e-02],\n         [ 3.34173627e-02, -2.57701986e-02, -2.08226591e-03, ...,\n          -3.81672308e-02,  3.72742377e-02,  5.84307052e-02],\n         [-3.05254310e-02, -1.41575336e-02,  4.63289432e-02, ...,\n           3.11797075e-02,  1.77289434e-02,  4.10461016e-02]],\n\n        [[-5.10600358e-02,  5.42509072e-02, -6.89703971e-04, ...,\n          -3.79039682e-02,  4.29699905e-02, -4.30909805e-02],\n         [-1.99785493e-02,  3.52671631e-02, -5.61943352e-02, ...,\n          -4.01885808e-02,  5.51172718e-03,  2.66741626e-02],\n         [-7.04827532e-03,  5.85093908e-02,  4.21230905e-02, ...,\n          -2.34619565e-02, -5.77718094e-02, -1.34788863e-02],\n         ...,\n         [-2.54554451e-02, -3.55947986e-02, -2.30057724e-02, ...,\n           5.14299758e-02, -1.77750550e-02, -5.36220446e-02],\n         [ 2.16647200e-02, -5.85373491e-02,  4.51355241e-02, ...,\n          -3.84649709e-02, -3.69175486e-02,  3.43742706e-02],\n         [-3.27188820e-02,  2.53531672e-02,  2.20822506e-02, ...,\n           5.07080369e-02, -2.83188745e-03, -1.01960115e-02]],\n\n        [[ 2.54969187e-02, -2.61611827e-02, -3.48166674e-02, ...,\n          -2.54312530e-02,  5.35753742e-03, -5.37595563e-02],\n         [-4.86933775e-02,  3.71298455e-02, -3.68448570e-02, ...,\n           9.54554603e-03, -7.39664584e-03,  1.31798498e-02],\n         [-3.83893475e-02, -5.46623841e-02, -4.05137986e-02, ...,\n          -5.30320741e-02,  1.22077055e-02,  4.72660176e-02],\n         ...,\n         [ 1.29704960e-02,  4.89976518e-02, -5.29685430e-02, ...,\n           7.35074654e-03, -2.63867527e-03,  8.54104385e-03],\n         [ 1.50210224e-02, -2.04025321e-02, -5.77369109e-02, ...,\n          -2.10241675e-02, -4.34633046e-02,  2.73389779e-02],\n         [-1.78920813e-02, -2.34989896e-02,  8.72182474e-03, ...,\n          -3.17422822e-02, -2.78989654e-02,  5.10256849e-02]]],\n\n\n       [[[ 5.16392477e-02,  5.61533310e-02,  9.08097252e-03, ...,\n           3.08794193e-02,  3.93535532e-02,  2.49830373e-02],\n         [ 2.79930644e-02, -4.14737687e-03, -1.11454092e-02, ...,\n           5.03407307e-02,  4.47459519e-04,  5.27900942e-02],\n         [ 3.52340564e-03, -7.22824037e-03,  2.77921930e-03, ...,\n          -4.80273589e-02, -5.18609397e-02, -2.40061283e-02],\n         ...,\n         [-2.89626047e-03,  4.55319025e-02,  4.90672477e-02, ...,\n          -5.69268353e-02, -5.39752766e-02,  5.38167246e-02],\n         [-2.30558030e-02,  3.07509862e-02,  4.74419408e-02, ...,\n          -4.16974649e-02, -2.29245052e-03,  1.77171864e-02],\n         [ 2.91716047e-02, -3.01435813e-02,  2.47952156e-02, ...,\n           2.62738951e-02,  1.46752112e-02, -3.56252976e-02]],\n\n        [[-1.55641846e-02, -2.96125300e-02,  1.23685263e-02, ...,\n          -5.72897904e-02,  5.09961434e-02,  5.38648926e-02],\n         [ 3.18529569e-02,  1.37805454e-02, -5.22247329e-03, ...,\n           4.17382084e-02, -3.17032672e-02, -9.25127417e-03],\n         [ 3.49958055e-02, -5.74504249e-02,  8.70924070e-03, ...,\n           4.63348962e-02, -1.20525807e-02,  5.52913807e-02],\n         ...,\n         [ 6.64596632e-03,  1.68264993e-02,  3.08705010e-02, ...,\n          -2.55147889e-02, -5.43857738e-02, -1.58110671e-02],\n         [-1.64798275e-03,  3.11265327e-02,  3.31513323e-02, ...,\n          -2.87190080e-04,  3.46353538e-02, -7.44935870e-03],\n         [-4.88969311e-03, -1.58995464e-02, -2.78077181e-02, ...,\n          -1.58512741e-02,  2.64122896e-02,  3.70878763e-02]],\n\n        [[ 5.39955869e-03,  5.30493669e-02, -4.24428172e-02, ...,\n          -1.84869990e-02,  3.90446968e-02,  3.97465415e-02],\n         [-4.13938090e-02,  2.99666934e-02, -5.67562245e-02, ...,\n          -8.98604468e-03,  2.97341831e-02,  1.71240084e-02],\n         [-2.29456425e-02, -2.82913670e-02,  4.04812060e-02, ...,\n           2.04350837e-02,  5.27900942e-02,  2.19430663e-02],\n         ...,\n         [-1.20399520e-03,  2.91409902e-02,  3.82960625e-02, ...,\n          -1.47794262e-02,  2.14670785e-02, -2.18186229e-02],\n         [ 3.92224230e-02, -4.78372350e-02,  4.43377718e-03, ...,\n           4.08553444e-02, -3.97521928e-02,  4.30302210e-02],\n         [ 5.45667671e-02, -2.98143439e-02,  4.19466384e-02, ...,\n          -5.21018058e-02,  4.69476990e-02, -2.58032978e-02]]]],\n      dtype=float32)>: ['conv2d_119/kernel']\n    <tf.Variable 'conv2d_119/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_119/bias']\n    <tf.Variable 'batch_normalization_134/gamma:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_134/gamma']\n    <tf.Variable 'batch_normalization_134/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_134/beta']\n    <tf.Variable 'batch_normalization_134/moving_mean:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_134/moving_mean']\n    <tf.Variable 'batch_normalization_134/moving_variance:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_134/moving_variance']\n    <tf.Variable 'conv2d_120/kernel:0' shape=(3, 3, 128, 128) dtype=float32, numpy=\narray([[[[ 4.93689999e-02,  9.58705693e-03,  4.25749421e-02, ...,\n           3.30835804e-02, -2.95136832e-02, -3.75214592e-02],\n         [-3.58970277e-02,  2.49863043e-02, -4.61151861e-02, ...,\n          -2.90487669e-02, -2.40574311e-02, -2.79105920e-02],\n         [ 4.20482159e-02,  3.01028565e-02,  2.46394128e-02, ...,\n          -4.49959897e-02,  1.32843852e-02,  4.52577695e-02],\n         ...,\n         [ 3.18974257e-04, -3.88700329e-02,  3.98853868e-02, ...,\n           7.24281743e-03,  3.16824913e-02, -2.71189753e-02],\n         [-2.23529916e-02, -7.32007623e-03, -4.96918336e-02, ...,\n           1.08009204e-02,  1.51445121e-02, -4.40491624e-02],\n         [-4.56182696e-02,  1.54934675e-02,  2.23518386e-02, ...,\n           4.82646376e-02,  2.31531113e-02, -4.55476716e-03]],\n\n        [[-4.38416824e-02, -9.82550159e-03,  4.86429855e-02, ...,\n           2.38651335e-02, -3.89891192e-02,  2.98683196e-02],\n         [-2.24120170e-03, -4.72485907e-02, -4.87625599e-02, ...,\n           4.59216833e-02,  1.60954967e-02,  1.05360150e-02],\n         [ 1.85889751e-03,  7.11571425e-04,  4.36385795e-02, ...,\n           4.85280305e-02,  3.53305712e-02,  9.72882286e-03],\n         ...,\n         [ 9.25891846e-03,  2.97392085e-02,  4.97144535e-02, ...,\n           1.90239772e-02,  1.78387761e-03,  1.01246685e-03],\n         [-1.95267797e-03, -1.62639990e-02,  3.62802669e-03, ...,\n          -2.50708610e-02, -1.51998438e-02,  1.76016241e-03],\n         [-4.98890132e-03, -2.12633386e-02, -1.01318695e-02, ...,\n          -2.78186835e-02, -6.25446439e-03,  9.86419618e-04]],\n\n        [[ 1.12398379e-02, -2.65389010e-02, -3.77460942e-02, ...,\n          -2.19314378e-02, -4.59022447e-02,  7.18642026e-04],\n         [ 3.50316465e-02,  4.11315411e-02, -3.09306215e-02, ...,\n           1.39028579e-02,  4.45092320e-02,  9.12372023e-04],\n         [-4.39234190e-02, -2.98041645e-02,  3.71468291e-02, ...,\n          -2.29838379e-02,  3.17297801e-02,  8.14467669e-03],\n         ...,\n         [-4.49046791e-02,  3.12201455e-02,  2.32214183e-02, ...,\n          -3.26759890e-02, -8.52194428e-03, -2.56756581e-02],\n         [ 2.93649957e-02, -4.58571762e-02,  2.63534039e-02, ...,\n           2.88554132e-02, -3.29503268e-02,  4.29437980e-02],\n         [ 2.12391838e-02,  2.14958340e-02, -2.33347509e-02, ...,\n           2.44273990e-02, -4.46736068e-03,  1.46632269e-03]]],\n\n\n       [[[-3.78015973e-02,  2.49770805e-02,  3.30687314e-03, ...,\n          -3.56002524e-03,  2.36704946e-02,  3.55348140e-02],\n         [-3.95305157e-02,  1.58600956e-02,  3.73867452e-02, ...,\n           5.20486012e-03, -3.64764929e-02, -1.48677155e-02],\n         [ 3.40450928e-02, -4.67161387e-02, -2.84752864e-02, ...,\n           1.30275786e-02,  3.03959325e-02, -2.61642020e-02],\n         ...,\n         [-6.10230491e-03, -2.25794148e-02, -3.66666839e-02, ...,\n          -5.08478917e-02,  5.09331599e-02, -1.99583694e-02],\n         [ 4.97555509e-02,  2.59329379e-02, -9.17588174e-03, ...,\n          -1.17883086e-04,  4.51394096e-02, -1.10434666e-02],\n         [ 2.85531655e-02,  1.90091655e-02,  2.63330787e-02, ...,\n           2.64201686e-02, -1.40645653e-02,  2.44300738e-02]],\n\n        [[-2.90244333e-02, -4.46649455e-02, -1.38321146e-03, ...,\n           2.27538049e-02,  4.43653166e-02, -2.98210513e-02],\n         [ 1.35339946e-02,  1.60827488e-02,  4.07807529e-03, ...,\n          -1.65734477e-02,  4.17180732e-03,  1.26213804e-02],\n         [-1.13680474e-02,  1.07784234e-02,  4.26524431e-02, ...,\n          -2.97750905e-03,  4.08553556e-02, -8.91075656e-03],\n         ...,\n         [-4.65830192e-02,  4.95095253e-02,  9.83839855e-03, ...,\n          -2.24753041e-02,  6.30866364e-03, -3.80909219e-02],\n         [ 4.10273224e-02, -5.23339212e-03,  2.23615691e-02, ...,\n           4.39599156e-03, -3.84214930e-02,  4.29538637e-02],\n         [-3.03713605e-03, -5.09993210e-02, -3.67709771e-02, ...,\n          -3.27449031e-02,  3.24407816e-02, -1.15534961e-02]],\n\n        [[ 3.90858129e-02,  1.09954923e-02,  3.10870111e-02, ...,\n           3.27078104e-02, -3.77341099e-02, -3.95357236e-02],\n         [ 3.35696489e-02,  4.06949595e-02, -1.76205523e-02, ...,\n          -4.97529693e-02,  2.10944861e-02, -3.40397507e-02],\n         [ 3.40584889e-02, -1.98929124e-02,  4.96087670e-02, ...,\n          -1.25674345e-02, -3.61455940e-02,  1.28065944e-02],\n         ...,\n         [-7.88710639e-03, -2.65663862e-02, -9.10404697e-03, ...,\n           3.00718173e-02, -3.55612859e-02,  2.65908614e-02],\n         [-4.98792119e-02,  6.97442889e-03, -1.65928900e-02, ...,\n          -4.17901017e-02,  4.37593386e-02, -2.14014202e-03],\n         [ 7.22844899e-03, -2.80509591e-02, -2.40052603e-02, ...,\n          -3.62379514e-02,  1.04710199e-02, -4.58389036e-02]]],\n\n\n       [[[ 4.45516035e-03, -4.04862650e-02, -1.11799501e-02, ...,\n           6.19007647e-03, -4.32470217e-02,  1.09133124e-03],\n         [-3.40234339e-02,  4.19235602e-02, -3.68703417e-02, ...,\n          -3.93697694e-02, -1.51001886e-02,  3.58697847e-02],\n         [ 1.18510798e-02,  7.32588023e-03,  2.50797570e-02, ...,\n          -3.77715081e-02,  2.50317305e-02,  2.89539769e-02],\n         ...,\n         [-2.16627605e-02,  2.85937786e-02, -4.73601110e-02, ...,\n          -3.69840041e-02,  2.47238129e-02, -4.06035781e-02],\n         [-2.33545583e-02,  2.13541389e-02,  1.81413144e-02, ...,\n          -1.07173100e-02, -4.58678119e-02,  3.59933674e-02],\n         [-3.20849046e-02, -4.51635569e-03, -4.44968417e-03, ...,\n          -1.88283809e-02,  1.18447915e-02,  4.98355553e-02]],\n\n        [[ 5.53007796e-03, -7.56491721e-03,  4.83432934e-02, ...,\n           1.61642432e-02,  1.29085332e-02, -4.67260890e-02],\n         [ 4.00211662e-03, -2.16754861e-02, -2.67323889e-02, ...,\n          -1.81723647e-02,  3.01630944e-02,  9.04835761e-05],\n         [ 4.37090173e-02, -1.19524896e-02, -4.73576039e-03, ...,\n           1.43546313e-02, -2.46655252e-02,  7.38089532e-03],\n         ...,\n         [-4.58623730e-02, -1.06066167e-02,  1.61216706e-02, ...,\n           2.01153010e-03,  3.18694338e-02, -5.03282622e-02],\n         [ 2.63516083e-02, -1.08181685e-03,  4.58064675e-02, ...,\n          -4.90407646e-03, -4.23228815e-02,  4.24849615e-03],\n         [ 1.68408081e-02, -4.53864932e-02, -4.39598709e-02, ...,\n          -2.80823261e-02,  1.74742714e-02, -4.84034084e-02]],\n\n        [[ 4.85124961e-02, -1.16197802e-02, -2.75778435e-02, ...,\n          -5.03165349e-02,  2.17136890e-02,  3.81449908e-02],\n         [-5.25261462e-03,  2.86215767e-02, -1.68121085e-02, ...,\n          -1.50543563e-02,  2.64202282e-02, -1.51182301e-02],\n         [-3.15402001e-02, -4.67997342e-02, -3.10818423e-02, ...,\n           4.74980846e-02, -2.22491845e-02, -1.44137405e-02],\n         ...,\n         [ 2.65321508e-03, -4.60932404e-03, -3.35533619e-02, ...,\n           3.27999368e-02,  2.38845795e-02, -3.95544730e-02],\n         [-3.97529379e-02, -4.67099585e-02, -7.71218538e-03, ...,\n           3.67036834e-03, -2.24653762e-02,  2.25502625e-02],\n         [-4.01150584e-02, -1.11637712e-02,  2.60866359e-02, ...,\n          -3.43745053e-02, -3.20409350e-02, -4.44150269e-02]]]],\n      dtype=float32)>: ['conv2d_120/kernel']\n    <tf.Variable 'conv2d_120/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_120/bias']\n    <tf.Variable 'batch_normalization_135/gamma:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_135/gamma']\n    <tf.Variable 'batch_normalization_135/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_135/beta']\n    <tf.Variable 'batch_normalization_135/moving_mean:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_135/moving_mean']\n    <tf.Variable 'batch_normalization_135/moving_variance:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_135/moving_variance']\n    <tf.Variable 'conv2d_121/kernel:0' shape=(3, 3, 128, 128) dtype=float32, numpy=\narray([[[[-0.01269745,  0.04905408,  0.02013322, ..., -0.00239365,\n           0.04028796,  0.0506104 ],\n         [ 0.01761603, -0.00885704, -0.03194425, ...,  0.02918463,\n           0.02075962,  0.01567951],\n         [-0.00478531, -0.02705448, -0.0421623 , ...,  0.04461244,\n          -0.00194044,  0.02606156],\n         ...,\n         [-0.02198179,  0.04070072,  0.00429545, ..., -0.04405405,\n          -0.01480733,  0.03806011],\n         [-0.00807586, -0.01942381, -0.00185722, ..., -0.03168061,\n          -0.00117967, -0.04921336],\n         [ 0.02640632, -0.0175267 , -0.04042577, ..., -0.03872542,\n           0.01161586, -0.03686703]],\n\n        [[-0.0187618 ,  0.02627771, -0.01435858, ..., -0.00417582,\n          -0.03776024,  0.04830144],\n         [ 0.00759704, -0.0006447 , -0.00274377, ...,  0.02343971,\n           0.02065577,  0.05051273],\n         [ 0.02344444,  0.02949789,  0.04057142, ..., -0.02869102,\n           0.04550803,  0.04740167],\n         ...,\n         [-0.00709274,  0.0081076 , -0.02629811, ..., -0.01269975,\n           0.02290613, -0.03631336],\n         [ 0.03634275, -0.01124256,  0.04328647, ...,  0.04003695,\n           0.00758653,  0.02584406],\n         [ 0.01550908, -0.05078228,  0.02146696, ..., -0.01670195,\n           0.01546928, -0.02428437]],\n\n        [[-0.03146384, -0.0006202 , -0.00566981, ..., -0.02294147,\n           0.00487546,  0.0480549 ],\n         [ 0.01846618, -0.02987513, -0.00930543, ..., -0.00318329,\n           0.00564116,  0.0497755 ],\n         [ 0.02111596,  0.02783251,  0.00626739, ...,  0.04759048,\n          -0.04127146,  0.01434208],\n         ...,\n         [ 0.02997809,  0.03887384, -0.04013238, ...,  0.01862207,\n           0.0119507 ,  0.04134411],\n         [ 0.01634627, -0.01655083,  0.04111227, ..., -0.02278326,\n           0.00080507, -0.05036426],\n         [-0.04078617,  0.02974917,  0.03301825, ..., -0.03375012,\n           0.03513197,  0.00281435]]],\n\n\n       [[[ 0.01391927, -0.02485917, -0.00090578, ...,  0.01682666,\n           0.02359542, -0.03122708],\n         [ 0.03192087, -0.02321855, -0.03504992, ..., -0.01961511,\n          -0.02472321,  0.00728473],\n         [ 0.01709327,  0.01605997,  0.03525992, ..., -0.01185247,\n           0.03889409,  0.0065316 ],\n         ...,\n         [-0.03872247, -0.00047392, -0.01674691, ...,  0.01983491,\n           0.00376344,  0.03968523],\n         [-0.00358749, -0.00097374,  0.02656883, ..., -0.03613694,\n          -0.01210178,  0.02947091],\n         [ 0.01258393,  0.01494206, -0.00203557, ...,  0.00016277,\n           0.02248247,  0.02864754]],\n\n        [[-0.01344074, -0.03173933, -0.00285281, ..., -0.04052752,\n           0.02927332,  0.02395136],\n         [ 0.01883733,  0.02292868, -0.01404168, ...,  0.0254643 ,\n          -0.01838596, -0.05001246],\n         [ 0.0052635 , -0.02194884, -0.01074829, ..., -0.00113347,\n          -0.01635316, -0.03385675],\n         ...,\n         [-0.00742848, -0.03546091,  0.04538983, ..., -0.03409924,\n           0.02153142,  0.0117134 ],\n         [ 0.03403291, -0.03202695,  0.01263048, ...,  0.03924796,\n          -0.01793033,  0.00832601],\n         [ 0.00595679, -0.0270025 , -0.03028694, ..., -0.03756505,\n           0.02006119, -0.02582492]],\n\n        [[-0.03894284, -0.01346277,  0.00651019, ...,  0.01963861,\n          -0.02323625,  0.04824764],\n         [ 0.00302097,  0.03847945, -0.0412301 , ..., -0.01259196,\n           0.00335873, -0.02051777],\n         [-0.02523535,  0.0223385 ,  0.02383216, ...,  0.01814397,\n           0.03761009, -0.00876028],\n         ...,\n         [-0.02082148, -0.04611816, -0.01451302, ...,  0.02439242,\n          -0.01394914, -0.00135339],\n         [-0.01188872,  0.04591449, -0.01310108, ...,  0.01864554,\n          -0.04902067, -0.03575729],\n         [-0.02070069,  0.00817444, -0.04962518, ..., -0.02482628,\n           0.03065899,  0.00570758]]],\n\n\n       [[[-0.02456442,  0.04740629, -0.04398212, ..., -0.05102671,\n          -0.0306594 , -0.04689203],\n         [-0.0131002 ,  0.02947746, -0.00924108, ...,  0.0143764 ,\n           0.01009108,  0.00772994],\n         [ 0.0045082 , -0.01006932, -0.04107497, ..., -0.00921172,\n          -0.03138926,  0.03753503],\n         ...,\n         [-0.0107894 , -0.01770936,  0.04865663, ..., -0.0365194 ,\n          -0.03847918,  0.02261656],\n         [-0.02107301,  0.00532807,  0.0497468 , ...,  0.03633555,\n          -0.03756252, -0.03185545],\n         [-0.04212245, -0.00360507, -0.02819018, ...,  0.00015112,\n          -0.04969614,  0.0300298 ]],\n\n        [[ 0.00645138,  0.02114152, -0.03259349, ...,  0.01052893,\n          -0.01070336,  0.02749782],\n         [-0.03409984,  0.0390601 ,  0.04490251, ...,  0.03037921,\n           0.01294971,  0.03409515],\n         [-0.04244926,  0.00166082,  0.03609452, ...,  0.02373201,\n           0.0492832 ,  0.00072695],\n         ...,\n         [-0.02338204, -0.00268789,  0.02987336, ..., -0.03632745,\n           0.00079149, -0.03510767],\n         [ 0.00861334,  0.03585543, -0.02030888, ...,  0.0505441 ,\n           0.03711492,  0.01128595],\n         [-0.03462014, -0.01242814, -0.02311822, ...,  0.0311583 ,\n          -0.04677533,  0.02265301]],\n\n        [[-0.0216049 , -0.04351216,  0.0123985 , ...,  0.02953226,\n           0.00624528, -0.01837631],\n         [ 0.04260097, -0.04299523, -0.0477579 , ..., -0.0012338 ,\n          -0.02355404,  0.0407764 ],\n         [ 0.02354145,  0.03925131, -0.03645778, ..., -0.02566555,\n          -0.01228136,  0.01681481],\n         ...,\n         [-0.04246598, -0.04090672, -0.02261275, ...,  0.0303967 ,\n           0.02297536, -0.02888449],\n         [ 0.02143775, -0.0424908 , -0.00735485, ..., -0.02875853,\n           0.00245093,  0.00794704],\n         [ 0.02735361,  0.03793734, -0.0091785 , ..., -0.02265177,\n           0.01899183,  0.02803575]]]], dtype=float32)>: ['conv2d_121/kernel']\n    <tf.Variable 'conv2d_121/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_121/bias']\n    <tf.Variable 'batch_normalization_136/gamma:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_136/gamma']\n    <tf.Variable 'batch_normalization_136/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_136/beta']\n    <tf.Variable 'batch_normalization_136/moving_mean:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_136/moving_mean']\n    <tf.Variable 'batch_normalization_136/moving_variance:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_136/moving_variance']\n    <tf.Variable 'dense_28/kernel:0' shape=(1152, 512) dtype=float32, numpy=\narray([[ 0.04191189,  0.0258666 ,  0.03149901, ...,  0.0442818 ,\n         0.04320597, -0.05236223],\n       [-0.02453576, -0.03397398,  0.05923217, ...,  0.01800971,\n        -0.03894537,  0.04898032],\n       [-0.03726929, -0.042066  , -0.02925297, ..., -0.01210067,\n        -0.03130576, -0.04250278],\n       ...,\n       [-0.04689163, -0.00256719,  0.04955404, ...,  0.05426704,\n        -0.03422888, -0.00604672],\n       [-0.01215716, -0.04747006,  0.00140962, ...,  0.01653213,\n        -0.02344661,  0.03255607],\n       [ 0.01943045,  0.0548328 ,  0.02283884, ..., -0.03332441,\n        -0.02933393,  0.03022568]], dtype=float32)>: ['dense_28/kernel']\n    <tf.Variable 'dense_28/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>: ['dense_28/bias']\n    <tf.Variable 'batch_normalization_137/gamma:0' shape=(512,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1.], dtype=float32)>: ['batch_normalization_137/gamma']\n    <tf.Variable 'batch_normalization_137/beta:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>: ['batch_normalization_137/beta']\n    <tf.Variable 'batch_normalization_137/moving_mean:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>: ['batch_normalization_137/moving_mean']\n    <tf.Variable 'batch_normalization_137/moving_variance:0' shape=(512,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1.], dtype=float32)>: ['batch_normalization_137/moving_variance']\n    <tf.Variable 'dense_29/kernel:0' shape=(512, 26) dtype=float32, numpy=\narray([[ 0.04909568,  0.00680554, -0.01343896, ...,  0.05586115,\n         0.09758975,  0.06862874],\n       [ 0.04505733,  0.09201207,  0.08753008, ...,  0.09166736,\n         0.00117949, -0.02747767],\n       [-0.05488635, -0.02415103, -0.02770717, ..., -0.02085422,\n         0.05624916, -0.05178185],\n       ...,\n       [-0.0919565 , -0.01380374, -0.09338132, ...,  0.02809937,\n        -0.03730787,  0.0370912 ],\n       [-0.04200146,  0.01515886, -0.00024891, ..., -0.05664562,\n         0.06770723, -0.02381963],\n       [ 0.02605527, -0.08068967,  0.05206772, ..., -0.05010242,\n        -0.00716743,  0.05964593]], dtype=float32)>: ['dense_29/kernel']\n    <tf.Variable 'dense_29/bias:0' shape=(26,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['dense_29/bias']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-282-d68c41c8c2f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cp.ckpt.index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2214\u001b[0m         \u001b[1;31m# streaming restore for any variables created in the future.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2215\u001b[0m         \u001b[0mtrackable_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreaming_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2216\u001b[1;33m       \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2217\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2218\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36massert_nontrivial_match\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;31m# assert_nontrivial_match and assert_consumed (and both are less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[1;31m# useful since we don't touch Python objects or Python state).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1023\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_consumed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_gather_saveable_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36massert_consumed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    996\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munused_attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m       ]\n\u001b[1;32m--> 998\u001b[1;33m       raise AssertionError(\n\u001b[0m\u001b[0;32m    999\u001b[0m           \"Some objects had attributes which were not restored:{}\".format(\n\u001b[0;32m   1000\u001b[0m               \"\".join(unused_attribute_strings)))\n",
      "\u001b[1;31mAssertionError\u001b[0m: Some objects had attributes which were not restored:\n    <tf.Variable 'conv2d_114/kernel:0' shape=(5, 5, 1, 32) dtype=float32, numpy=\narray([[[[-5.14483005e-02, -5.09646162e-02,  2.01726556e-02,\n           7.85938650e-02,  3.67258340e-02, -7.30120391e-03,\n          -5.93751147e-02,  7.33189285e-02, -8.31270590e-02,\n          -3.88594344e-02,  3.89861465e-02,  1.62232667e-02,\n          -6.72789067e-02, -7.12333396e-02, -3.41200717e-02,\n           6.31483197e-02,  4.38336730e-02,  7.46641606e-02,\n          -8.36632103e-02,  5.37628382e-02, -4.50131372e-02,\n           2.38340348e-03, -5.71911484e-02, -1.40122622e-02,\n          -3.33793201e-02,  7.01417178e-02,  5.02503514e-02,\n           2.21611410e-02, -7.90058076e-03, -1.52141675e-02,\n           5.55222481e-02, -2.85171717e-02]],\n\n        [[-1.67478845e-02, -2.93735936e-02, -6.30529225e-02,\n          -4.31816764e-02,  8.84154439e-03,  1.84094086e-02,\n          -7.64121637e-02, -7.32967705e-02,  4.75377440e-02,\n          -4.62013036e-02, -6.51193634e-02, -4.07987162e-02,\n          -2.61693075e-02, -3.33856642e-02,  8.99508595e-05,\n           4.15581316e-02,  7.23612159e-02,  7.55306333e-03,\n          -6.76482022e-02, -1.72045901e-02, -6.42681047e-02,\n           2.92497873e-03,  6.99493438e-02, -5.57070673e-02,\n          -8.36358592e-02, -4.75126803e-02, -1.41796172e-02,\n          -4.88877818e-02, -8.21464509e-02,  8.44985396e-02,\n           6.94993436e-02,  5.51679283e-02]],\n\n        [[ 6.76997751e-03,  1.82163268e-02, -6.91296458e-02,\n          -3.90797965e-02, -1.78025216e-02,  3.28022987e-03,\n          -5.26482202e-02, -7.56224319e-02,  8.05902332e-02,\n          -7.35237598e-02,  8.31596553e-04,  5.22324145e-02,\n          -3.41160037e-02, -2.97664106e-03,  9.39517468e-03,\n          -3.05165388e-02,  8.24845731e-02,  5.36263287e-02,\n           6.09951168e-02, -6.98284507e-02,  3.22947726e-02,\n           7.79469311e-03, -5.61252013e-02,  2.67461166e-02,\n           7.04264045e-02,  5.80172539e-02,  3.37229371e-02,\n           7.27994442e-02,  3.80280018e-02, -6.62386417e-04,\n           2.43976638e-02, -4.94443402e-02]],\n\n        [[ 4.94262874e-02, -2.59392075e-02, -7.12682679e-02,\n           5.91860265e-02,  7.99528509e-02, -6.97373599e-02,\n           1.65382549e-02,  5.92710525e-02, -5.18307760e-02,\n           7.09618032e-02,  7.42624998e-02, -6.47425056e-02,\n          -1.34686530e-02, -4.05449681e-02, -1.62181035e-02,\n          -8.22874904e-03,  5.15605211e-02,  1.91978961e-02,\n          -8.27538595e-02,  2.23291069e-02, -7.14225471e-02,\n           1.30284354e-02,  5.53124994e-02,  5.38352728e-02,\n          -4.44851033e-02,  4.44162339e-02,  1.06921792e-03,\n          -1.26556829e-02,  5.21764010e-02,  5.56270033e-03,\n           2.45717168e-03, -4.56350856e-02]],\n\n        [[-7.69578442e-02,  7.14776367e-02, -6.31289408e-02,\n          -8.35320875e-02, -6.34422451e-02,  6.78313971e-02,\n           4.53344285e-02, -1.02100596e-02, -6.93563074e-02,\n          -2.75498591e-02,  3.55297774e-02,  8.39604288e-02,\n          -7.61588439e-02,  5.95747679e-03,  5.80256879e-02,\n           3.34110633e-02, -2.87995487e-02,  3.92788872e-02,\n           6.50151670e-02, -8.26600865e-02,  1.52926743e-02,\n          -5.40460311e-02, -2.78106332e-03,  6.46309257e-02,\n          -3.56427059e-02, -7.84478411e-02,  1.69967338e-02,\n           6.80284500e-02,  3.58877927e-02,  9.83889401e-03,\n           4.42317426e-02,  3.61514837e-02]]],\n\n\n       [[[-5.19356504e-02, -5.79366088e-03, -3.06328833e-02,\n           5.90210855e-02,  1.16472170e-02, -1.07601956e-02,\n          -6.21170402e-02, -1.77982748e-02, -8.25423449e-02,\n          -6.54612556e-02, -2.31650956e-02,  3.84567678e-02,\n          -3.18618752e-02, -7.05977231e-02, -6.90786913e-02,\n          -7.82714412e-02, -2.62098722e-02,  3.04181501e-02,\n          -5.22549897e-02,  5.11376858e-02, -1.07261762e-02,\n           8.45154971e-02,  7.62859434e-02, -6.10446185e-03,\n          -9.54426825e-04,  5.56863844e-03,  5.31065166e-02,\n           3.66170928e-02,  6.53384626e-02,  4.05988842e-02,\n           5.02419621e-02,  2.33724862e-02]],\n\n        [[ 4.98488098e-02,  3.64903361e-03,  2.66331509e-02,\n           4.67141122e-02, -4.99502905e-02,  1.80221573e-02,\n          -6.75055385e-02, -1.90874338e-02, -6.09182417e-02,\n          -3.02487016e-02,  1.73180625e-02,  7.86809325e-02,\n          -6.30562752e-03,  1.36207193e-02,  4.15116698e-02,\n           1.55274123e-02, -3.40723097e-02, -4.50498611e-03,\n          -6.37702644e-02, -3.30978408e-02,  5.30916899e-02,\n           6.31577075e-02, -3.69128510e-02, -5.05554453e-02,\n          -2.46887021e-02, -2.89748609e-03, -2.89413258e-02,\n          -1.26375854e-02, -3.80911119e-02, -6.40609413e-02,\n          -2.64168344e-02, -4.76165190e-02]],\n\n        [[ 3.13770249e-02, -3.91848944e-02, -8.34010392e-02,\n          -6.69360608e-02,  7.36917108e-02,  7.12070614e-02,\n           1.68437064e-02,  1.21146366e-02,  2.05407888e-02,\n           5.39576858e-02,  3.48509401e-02, -7.42598325e-02,\n          -1.48426369e-02,  1.29753500e-02, -4.52577360e-02,\n          -7.17102364e-02, -2.65413933e-02,  1.63334236e-02,\n           4.20787632e-02,  8.54030252e-03, -8.04318637e-02,\n          -6.47110865e-02, -2.67023034e-02,  6.85000718e-02,\n          -4.32888493e-02,  4.24836427e-02, -3.35543603e-03,\n          -9.29804891e-03,  5.04013002e-02, -1.32977962e-02,\n          -8.15028697e-02, -9.92459059e-03]],\n\n        [[-2.67780200e-02,  3.68281230e-02, -6.02224693e-02,\n          -3.28734927e-02, -3.49213295e-02, -1.93966255e-02,\n          -1.82527006e-02,  3.07456851e-02, -3.62134762e-02,\n          -1.32801682e-02, -7.43378922e-02,  4.78242934e-02,\n          -3.74347009e-02, -3.73389348e-02, -1.35883689e-03,\n           6.66925609e-02, -3.24662030e-03,  4.76544648e-02,\n           2.11831927e-02, -2.57394016e-02,  3.57929841e-02,\n          -5.46210110e-02, -8.11140314e-02,  6.16969913e-02,\n          -5.29541001e-02,  2.44519934e-02, -3.59662138e-02,\n           2.22078264e-02,  6.68109655e-02, -4.04674634e-02,\n           8.40559602e-04,  5.59461713e-02]],\n\n        [[-8.42168927e-04,  5.96341491e-03,  2.34083086e-03,\n           2.83593535e-02,  2.97983736e-03, -6.10663742e-03,\n           3.22834477e-02,  1.65871754e-02, -1.85287148e-02,\n          -8.00915137e-02, -3.40600312e-03,  1.31545961e-02,\n          -4.54472005e-03,  5.71999997e-02,  7.94627815e-02,\n          -5.39776944e-02, -2.04629004e-02,  7.50243664e-04,\n           6.39799088e-02,  6.71744347e-04,  7.67388642e-02,\n           1.91841125e-02, -3.56938615e-02,  5.73180616e-02,\n          -3.42374295e-02, -2.28424408e-02, -7.03058988e-02,\n          -2.50717439e-02, -2.07634717e-02,  1.63123012e-02,\n           1.65876001e-03, -6.97200000e-03]]],\n\n\n       [[[ 7.08167106e-02, -4.15653102e-02, -7.04499334e-02,\n           1.08586028e-02, -7.76553452e-02,  1.66892633e-02,\n          -2.55586244e-02, -5.44908047e-02,  2.38228068e-02,\n          -4.80652563e-02, -7.73183778e-02, -4.27093729e-02,\n           5.44033051e-02, -5.16761877e-02,  1.35839954e-02,\n          -2.96065956e-03,  4.14696634e-02, -7.54994825e-02,\n          -3.70977111e-02,  1.53204650e-02,  1.24688670e-02,\n           6.95328712e-02,  3.21613997e-03, -2.37783007e-02,\n           3.92514840e-02, -4.81264144e-02,  5.61483204e-02,\n          -2.73631476e-02, -7.92394876e-02,  2.09793821e-02,\n           3.23177502e-02, -4.39836457e-02]],\n\n        [[-7.25063086e-02, -8.43549147e-02,  4.01187390e-02,\n           4.84688580e-02, -4.24608290e-02,  1.86926350e-02,\n          -3.94862220e-02, -1.27617568e-02, -3.46452184e-02,\n           5.78139275e-02, -7.85863474e-02,  1.85164735e-02,\n           4.02261615e-02,  1.72825605e-02, -3.00131291e-02,\n           2.94023007e-03, -3.68990004e-04, -8.01117495e-02,\n          -1.11638308e-02,  2.67949179e-02,  9.90511477e-03,\n           8.38044137e-02, -1.85889006e-03,  2.51948982e-02,\n           1.25745684e-03,  4.01238650e-02,  1.64795369e-02,\n          -1.13520250e-02, -1.53782740e-02,  7.42655545e-02,\n           2.94291228e-03,  6.54212236e-02]],\n\n        [[ 4.07826751e-02,  4.22167629e-02,  7.05054402e-02,\n          -3.76420319e-02,  6.53587431e-02, -5.09557500e-02,\n          -7.03775063e-02, -2.10969150e-03, -1.13606676e-02,\n           2.80547142e-02, -7.23839477e-02,  1.72951818e-03,\n           3.58502343e-02,  5.92166185e-02, -7.02653974e-02,\n          -1.88585520e-02, -4.46446948e-02,  7.64969885e-02,\n           2.98979878e-03,  6.39982820e-02, -5.42797893e-03,\n          -7.13942647e-02,  5.48530817e-02,  2.97057256e-02,\n           3.33500206e-02, -2.58450955e-03,  2.59884447e-03,\n          -4.25752401e-02, -1.69291869e-02, -3.04224417e-02,\n          -8.34372714e-02, -1.80972219e-02]],\n\n        [[ 6.91497177e-02,  3.75580788e-02, -8.32781717e-02,\n          -2.14714631e-02,  2.32067183e-02,  5.96312135e-02,\n           6.72828108e-02,  3.17399576e-02,  7.50948638e-02,\n          -8.08411092e-02, -4.26320098e-02,  7.95608461e-02,\n           6.59239292e-03, -4.04824317e-03,  1.03071481e-02,\n           3.99650633e-02,  7.70720094e-02, -1.56719908e-02,\n          -4.49009016e-02, -1.95531473e-02,  4.65287119e-02,\n          -6.13850951e-02, -6.48038238e-02, -1.47439241e-02,\n           1.48249045e-02,  1.74490884e-02, -5.67066669e-02,\n          -1.75460428e-03, -4.39291969e-02, -4.25253659e-02,\n           4.03095186e-02,  1.55874491e-02]],\n\n        [[ 8.05509239e-02, -3.29339616e-02, -6.21445104e-02,\n          -3.46073397e-02, -2.49367580e-02,  8.49510282e-02,\n           7.81956017e-02,  1.72890723e-03, -1.14329532e-02,\n          -2.12855488e-02, -2.35341489e-02,  4.51733619e-02,\n          -3.50398272e-02, -4.03717905e-03,  3.42795625e-02,\n          -8.39426145e-02,  6.45890981e-02,  7.47614950e-02,\n          -1.47046596e-02,  3.59004959e-02,  2.65332982e-02,\n          -2.23290473e-02,  3.14059183e-02, -1.71203911e-02,\n           5.48159182e-02,  4.65227365e-02, -7.16410205e-02,\n           2.45064870e-02,  3.25034186e-02, -8.32620934e-02,\n          -4.80079986e-02,  5.76639026e-02]]],\n\n\n       [[[ 7.70103633e-02, -2.38308311e-03,  2.46418789e-02,\n          -8.20644498e-02, -4.60724160e-02, -6.45603240e-02,\n           7.53006339e-03,  3.62473875e-02,  5.33515960e-02,\n           5.53974509e-02,  2.13695318e-03, -5.81137724e-02,\n          -3.31136398e-02, -1.70647651e-02,  2.97719687e-02,\n          -3.31601165e-02,  6.23286366e-02,  3.70657444e-03,\n          -6.69322610e-02,  6.80418015e-02, -5.47790974e-02,\n          -8.23242962e-03,  4.75444794e-02,  7.67481625e-02,\n          -1.03140771e-02, -7.64542818e-03,  7.64319748e-02,\n          -8.05685967e-02,  1.45463273e-02,  6.21394217e-02,\n           5.94590008e-02,  8.42186958e-02]],\n\n        [[-2.15172991e-02, -8.09834152e-02,  6.51657432e-02,\n          -2.89698727e-02, -4.30320501e-02,  3.41561586e-02,\n          -2.63939612e-02, -5.95750436e-02, -1.70316547e-03,\n           4.45508361e-02,  6.54877722e-02,  1.26032680e-02,\n           2.56539658e-02,  1.94653496e-02, -6.94301799e-02,\n           7.18882680e-03,  5.47480285e-02,  3.41875926e-02,\n          -5.22099957e-02,  6.83116466e-02,  1.15721449e-02,\n           6.75344616e-02,  7.45189339e-02, -4.84893881e-02,\n          -8.37121904e-02,  7.83386528e-02,  7.17304498e-02,\n          -4.39944118e-03,  7.97368139e-02, -4.56971601e-02,\n           1.81042179e-02, -2.48248279e-02]],\n\n        [[-1.93358138e-02,  5.23116440e-02, -2.67364830e-02,\n           8.62997025e-03,  2.20735520e-02,  6.86911643e-02,\n           3.95361334e-02, -1.11933574e-02,  8.24351162e-02,\n           1.17632896e-02,  3.30266133e-02, -8.37104395e-02,\n          -6.92209154e-02,  9.80630517e-04,  4.35330719e-03,\n          -2.49019079e-02, -5.20139188e-03,  2.33110413e-02,\n          -7.84608573e-02, -2.24735662e-02, -7.22100288e-02,\n           2.33903602e-02,  5.70264906e-02, -4.79246154e-02,\n           6.37671798e-02, -2.93696895e-02,  4.20370698e-02,\n           7.51633644e-02,  3.63559872e-02,  7.28204846e-03,\n           1.46139786e-02, -6.13747239e-02]],\n\n        [[ 5.50170839e-02,  2.60977373e-02, -3.22272070e-02,\n           6.35971725e-02, -2.94441059e-02, -6.68592677e-02,\n          -1.59639269e-02,  1.69506595e-02, -6.99899495e-02,\n          -4.00409847e-03,  4.86414582e-02,  7.02627301e-02,\n           5.43139875e-02, -5.58452681e-02, -1.97886378e-02,\n          -2.25468054e-02,  5.79256117e-02,  4.24005687e-02,\n          -3.75262573e-02, -8.29783529e-02, -9.47709382e-03,\n           4.11262065e-02, -2.48581544e-02,  1.35859698e-02,\n           2.46383622e-02,  6.62097931e-02,  2.40958109e-02,\n           6.04675114e-02, -5.47211096e-02,  1.94521546e-02,\n          -6.63882494e-02, -8.16585347e-02]],\n\n        [[-7.21692964e-02, -4.82497290e-02, -1.13845393e-02,\n          -5.40195405e-03,  1.02636367e-02, -2.32595168e-02,\n           3.19817960e-02,  8.37199688e-02,  7.81929344e-02,\n           5.00818640e-02, -6.01271912e-02, -7.89984688e-02,\n           5.86514026e-02,  7.50228912e-02, -4.57147881e-02,\n           2.97245309e-02,  2.71621570e-02, -2.67836303e-02,\n          -1.97186917e-02, -3.08973640e-02, -3.15042660e-02,\n           3.43058258e-03, -1.57626346e-02, -1.01739466e-02,\n          -5.91270998e-02, -1.27623230e-02, -1.07187182e-02,\n           3.04223374e-02, -8.31662416e-02,  8.15691501e-02,\n          -1.16887316e-02, -7.88021758e-02]]],\n\n\n       [[[ 3.78468186e-02,  2.60055289e-02,  6.77331388e-02,\n          -2.69781724e-02,  7.56607056e-02, -6.53923079e-02,\n           2.70674303e-02, -9.51802731e-03,  4.94771302e-02,\n          -6.56722635e-02, -2.57047527e-02,  6.60489053e-02,\n           1.02866739e-02,  6.52538985e-02,  6.01759404e-02,\n           2.47062743e-03, -8.21836293e-04,  3.87634262e-02,\n           7.41065294e-03, -7.94431567e-03, -7.31641054e-02,\n           2.10043788e-03,  3.45099270e-02, -2.67503485e-02,\n           4.08622622e-02, -5.35255410e-02,  6.95426613e-02,\n          -7.54985511e-02,  3.64117175e-02,  1.60108134e-02,\n           1.08646601e-02, -4.57864814e-02]],\n\n        [[ 8.24451745e-02, -6.59886971e-02, -4.71873432e-02,\n           1.42157078e-02, -8.01478997e-02,  4.07901853e-02,\n           5.18690646e-02,  2.77418569e-02,  2.56453827e-02,\n           3.49805206e-02, -4.38251160e-02,  8.16472918e-02,\n          -2.08518803e-02, -8.46109390e-02, -6.42269701e-02,\n          -8.41284096e-02,  1.16627887e-02, -3.73849459e-02,\n          -6.10176250e-02, -7.14108199e-02, -7.79422373e-02,\n           8.70229304e-03, -3.99920903e-02,  1.27683654e-02,\n           7.32441992e-02, -8.22008178e-02, -2.71821767e-03,\n           2.80757770e-02, -6.50995374e-02,  6.37632608e-03,\n          -2.50077397e-02, -3.62034924e-02]],\n\n        [[ 4.63374853e-02,  3.70459482e-02,  4.47778106e-02,\n          -6.80370852e-02,  3.68895903e-02,  5.98122627e-02,\n          -6.04536459e-02,  8.46938789e-02, -1.89360380e-02,\n          -6.41082376e-02, -1.93697661e-02,  6.50997013e-02,\n          -3.50221172e-02, -4.50141318e-02,  4.45175171e-02,\n           6.06471747e-02, -3.38744558e-02,  2.58439109e-02,\n           3.99917662e-02,  6.70125484e-02, -4.55719121e-02,\n           6.94085807e-02, -7.83388019e-02, -5.83933219e-02,\n           5.31747639e-02,  1.69842690e-02,  4.67295945e-02,\n          -5.18412255e-02,  1.09351724e-02,  5.12499809e-02,\n           6.32988364e-02,  8.32819194e-03]],\n\n        [[ 2.70534232e-02,  2.80291513e-02,  1.71221048e-03,\n           3.55504155e-02,  7.85848498e-02,  2.45471075e-02,\n          -7.06288144e-02,  2.18944028e-02,  7.15783387e-02,\n           6.70951009e-02, -5.75722381e-02,  7.74262697e-02,\n          -3.21663506e-02,  2.85557434e-02, -5.27653322e-02,\n           3.15729678e-02, -6.73605055e-02,  4.71258014e-02,\n          -4.70406562e-03, -9.84051824e-03,  7.10418671e-02,\n          -4.95103188e-02,  1.77704617e-02,  1.58598423e-02,\n          -2.16057822e-02, -6.13741130e-02,  7.65690655e-02,\n          -8.25137645e-03, -6.36689365e-04, -1.99301243e-02,\n          -2.60162652e-02,  2.73930728e-02]],\n\n        [[ 5.82411587e-02,  2.84197628e-02,  8.34772438e-02,\n           6.68082386e-03, -1.87359005e-03,  2.99061835e-02,\n           7.09632635e-02,  5.64216077e-02,  4.17516232e-02,\n           8.25207978e-02, -4.87305112e-02, -1.94430649e-02,\n          -1.22744069e-02,  6.71185255e-02,  7.48435855e-02,\n          -1.15827844e-02,  5.71184456e-02,  2.43371353e-02,\n           5.16017973e-02,  4.44789231e-02,  5.22666574e-02,\n          -6.94158077e-02, -2.76609138e-02,  5.45022041e-02,\n           7.22679496e-02,  2.92573571e-02, -3.59787792e-02,\n          -2.49298252e-02, -6.37726635e-02,  7.47311115e-03,\n           7.76798129e-02,  2.46423632e-02]]]], dtype=float32)>: ['conv2d_114/kernel']\n    <tf.Variable 'conv2d_114/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['conv2d_114/bias']\n    <tf.Variable 'batch_normalization_129/gamma:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_129/gamma']\n    <tf.Variable 'batch_normalization_129/beta:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_129/beta']\n    <tf.Variable 'batch_normalization_129/moving_mean:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_129/moving_mean']\n    <tf.Variable 'batch_normalization_129/moving_variance:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_129/moving_variance']\n    <tf.Variable 'conv2d_115/kernel:0' shape=(3, 3, 32, 32) dtype=float32, numpy=\narray([[[[ 0.08894353, -0.05977122,  0.05894132, ..., -0.04162906,\n           0.02519324, -0.09510606],\n         [ 0.03003617,  0.03976461,  0.05913787, ...,  0.04029824,\n          -0.00649183,  0.00264079],\n         [-0.00435548, -0.06751756, -0.10098987, ...,  0.06662126,\n          -0.08595265,  0.062951  ],\n         ...,\n         [-0.08272442,  0.05674174,  0.04043259, ...,  0.08818418,\n           0.08564639, -0.0828737 ],\n         [-0.09780145,  0.00107975,  0.03474534, ...,  0.05599388,\n           0.0935244 ,  0.07212874],\n         [ 0.07871538, -0.02359098,  0.09242372, ...,  0.05296732,\n           0.05820541,  0.02591023]],\n\n        [[-0.04237591,  0.10151696,  0.05251798, ..., -0.02696823,\n          -0.00659166,  0.06681097],\n         [-0.06719928, -0.05489751, -0.01801632, ..., -0.04680521,\n           0.09253623,  0.06679554],\n         [ 0.06629281,  0.09349979,  0.06026611, ...,  0.08937   ,\n          -0.05042158,  0.00784368],\n         ...,\n         [ 0.08094649, -0.02040904,  0.04319829, ..., -0.05099342,\n           0.07327187,  0.03586023],\n         [-0.01352485, -0.06036359, -0.09180471, ..., -0.00549743,\n          -0.02357962, -0.10205168],\n         [-0.07485071, -0.08910556,  0.00868326, ...,  0.00876678,\n          -0.10099641, -0.04902995]],\n\n        [[ 0.03439131,  0.04943204, -0.0916671 , ..., -0.00508028,\n           0.0555833 , -0.10143805],\n         [ 0.09338669,  0.10032207,  0.0538581 , ..., -0.00236005,\n           0.00637615,  0.0784574 ],\n         [ 0.04138272, -0.04026536, -0.05924457, ...,  0.0671355 ,\n           0.09607543, -0.03936634],\n         ...,\n         [-0.00871604, -0.03516168,  0.04433656, ..., -0.05127683,\n          -0.02237319, -0.075156  ],\n         [-0.09733602, -0.03928996, -0.09156368, ...,  0.01875317,\n          -0.03495908, -0.05530023],\n         [-0.04071162, -0.01651368,  0.02589984, ..., -0.03172555,\n           0.0981721 , -0.00238522]]],\n\n\n       [[[ 0.0620397 , -0.01395828,  0.04724044, ...,  0.00929736,\n          -0.04187829,  0.07943258],\n         [ 0.09680519, -0.03939717, -0.0457579 , ..., -0.02058422,\n          -0.03641162, -0.06849881],\n         [ 0.07510222,  0.06301501, -0.01792016, ..., -0.07129081,\n          -0.06565954, -0.00864362],\n         ...,\n         [ 0.05879474, -0.07695948, -0.03712591, ..., -0.07289186,\n          -0.03883222,  0.03090015],\n         [-0.09596459,  0.04458947,  0.07616664, ..., -0.07194636,\n          -0.07698016, -0.08578504],\n         [-0.0346738 , -0.09395652, -0.09684777, ...,  0.07793161,\n           0.01901412,  0.03697993]],\n\n        [[-0.07912328,  0.06141171, -0.09213284, ...,  0.06304573,\n           0.01559288, -0.02195154],\n         [ 0.0148547 ,  0.08717996, -0.04478246, ...,  0.08632775,\n           0.10205236, -0.07149363],\n         [-0.04560793,  0.03222919,  0.09684677, ..., -0.01836649,\n          -0.09414296, -0.02157916],\n         ...,\n         [ 0.06407459,  0.05868796, -0.04207882, ..., -0.08945329,\n          -0.02850287,  0.08666797],\n         [-0.0564331 ,  0.02840702,  0.05653425, ..., -0.06775163,\n          -0.09597824,  0.07533146],\n         [ 0.03162269,  0.05734962, -0.04673111, ..., -0.02168647,\n          -0.09085219,  0.03113756]],\n\n        [[ 0.03547588, -0.02106884,  0.03564803, ..., -0.03206116,\n          -0.02762562, -0.06682092],\n         [ 0.05365343, -0.08919559, -0.06454669, ...,  0.08827245,\n          -0.04603099,  0.09574811],\n         [-0.06542459, -0.04988228,  0.05780156, ..., -0.04713773,\n           0.07599083, -0.04758558],\n         ...,\n         [ 0.09420523, -0.03136744,  0.02756457, ...,  0.0258453 ,\n          -0.09475578,  0.04846892],\n         [ 0.03974558,  0.00399264, -0.03045838, ..., -0.06767476,\n          -0.03899238,  0.09514453],\n         [-0.08320564, -0.05849544,  0.08917265, ...,  0.04358241,\n          -0.06216218, -0.02731756]]],\n\n\n       [[[ 0.05137509, -0.01093893, -0.01660357, ...,  0.08871579,\n          -0.07828309,  0.06777547],\n         [ 0.03767197, -0.06856626, -0.01336688, ...,  0.01829906,\n           0.07840005,  0.02403153],\n         [-0.01591724, -0.00534578,  0.00113757, ...,  0.04423308,\n          -0.00991281, -0.06822645],\n         ...,\n         [ 0.07546867,  0.04941963,  0.02619034, ...,  0.10102426,\n           0.06489037,  0.03450869],\n         [ 0.0395615 , -0.04127698, -0.09556214, ..., -0.0048686 ,\n           0.02355589,  0.0869129 ],\n         [-0.03414661, -0.04860034,  0.06294554, ..., -0.090893  ,\n           0.05675966, -0.01181561]],\n\n        [[-0.06262171,  0.01091676,  0.0903264 , ..., -0.04323716,\n           0.00634228,  0.03258082],\n         [-0.06421553,  0.10183108, -0.08845873, ...,  0.07690609,\n           0.07459472, -0.00943173],\n         [-0.02010056, -0.078018  , -0.09281311, ...,  0.0975914 ,\n          -0.0802223 , -0.01641832],\n         ...,\n         [-0.08029749, -0.05407443,  0.05013904, ..., -0.05646541,\n          -0.04274823,  0.01729671],\n         [-0.03296327, -0.09287234, -0.0129387 , ..., -0.04301468,\n          -0.07404072, -0.01288386],\n         [-0.0986732 ,  0.00915796, -0.03652263, ..., -0.05706514,\n          -0.0150627 , -0.08883035]],\n\n        [[ 0.03350657,  0.09461167,  0.01703417, ...,  0.06641054,\n          -0.03846505, -0.08721709],\n         [-0.01377843, -0.01191448,  0.07536598, ...,  0.09785073,\n          -0.01292726,  0.08728831],\n         [ 0.09276715, -0.01423037, -0.06214957, ...,  0.03962807,\n           0.10047069, -0.04038662],\n         ...,\n         [-0.08180322, -0.02183474,  0.08713834, ...,  0.08776809,\n           0.00130038,  0.04784954],\n         [-0.03813103, -0.0397272 ,  0.02303384, ...,  0.0239353 ,\n          -0.0607739 ,  0.09779748],\n         [-0.01564303, -0.06503414,  0.01879495, ...,  0.00814204,\n          -0.04783155, -0.08780671]]]], dtype=float32)>: ['conv2d_115/kernel']\n    <tf.Variable 'conv2d_115/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['conv2d_115/bias']\n    <tf.Variable 'batch_normalization_130/gamma:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_130/gamma']\n    <tf.Variable 'batch_normalization_130/beta:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_130/beta']\n    <tf.Variable 'batch_normalization_130/moving_mean:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>: ['batch_normalization_130/moving_mean']\n    <tf.Variable 'batch_normalization_130/moving_variance:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>: ['batch_normalization_130/moving_variance']\n    <tf.Variable 'conv2d_116/kernel:0' shape=(3, 3, 32, 64) dtype=float32, numpy=\narray([[[[ 0.06786553, -0.0261334 ,  0.05468185, ...,  0.05123378,\n           0.04701877, -0.04092249],\n         [-0.00363847, -0.01574651, -0.04981294, ..., -0.04060807,\n          -0.07870933, -0.01496635],\n         [ 0.06977115, -0.06160953,  0.06276996, ..., -0.01680875,\n           0.0556236 ,  0.00582371],\n         ...,\n         [ 0.053202  , -0.04889594,  0.04198524, ...,  0.01900645,\n          -0.0331565 ,  0.01057426],\n         [-0.0501347 , -0.03942879,  0.01960655, ...,  0.01956026,\n          -0.05039076, -0.04256004],\n         [-0.06611538, -0.06159077, -0.07272055, ..., -0.023371  ,\n          -0.0026655 , -0.00966045]],\n\n        [[ 0.06185357,  0.08331808, -0.00642395, ...,  0.06653073,\n          -0.06299537,  0.00163712],\n         [ 0.05239757, -0.0769107 ,  0.06467684, ..., -0.07825466,\n          -0.03137428,  0.00842508],\n         [ 0.07060862, -0.00917542, -0.00881752, ..., -0.04073662,\n          -0.01588174, -0.01227534],\n         ...,\n         [-0.04347279,  0.05009105,  0.0205672 , ...,  0.06649119,\n           0.0721819 ,  0.00302943],\n         [-0.05579249,  0.06492988,  0.00272594, ...,  0.00871736,\n          -0.01420721,  0.04369409],\n         [ 0.05928604, -0.03155796, -0.0394375 , ..., -0.0519192 ,\n           0.0780837 ,  0.0127368 ]],\n\n        [[ 0.02232895, -0.03892915, -0.07620295, ..., -0.07362779,\n           0.02034465,  0.04515404],\n         [ 0.04273409, -0.00619683,  0.02169609, ...,  0.06995317,\n           0.02664389,  0.04807133],\n         [-0.04848013,  0.05660107,  0.0179223 , ..., -0.01286534,\n          -0.043562  ,  0.00670568],\n         ...,\n         [ 0.0466852 ,  0.04046664, -0.03091624, ..., -0.03739556,\n           0.04801627,  0.005681  ],\n         [-0.03502915, -0.02775478,  0.06107005, ..., -0.02930908,\n           0.04779597,  0.05749723],\n         [ 0.04604412,  0.07484207,  0.04582614, ...,  0.01780865,\n          -0.08090061, -0.07922006]]],\n\n\n       [[[ 0.05491076, -0.00425478, -0.02198982, ...,  0.0025504 ,\n           0.01308123, -0.02995614],\n         [-0.03144783, -0.05735376, -0.03113335, ...,  0.01946803,\n          -0.05290896, -0.02477866],\n         [-0.0105044 ,  0.0075049 , -0.01178054, ..., -0.00464308,\n           0.01379702, -0.03651337],\n         ...,\n         [-0.03535529,  0.03391626, -0.05554207, ..., -0.0733374 ,\n           0.0512488 , -0.02560636],\n         [ 0.07440766,  0.05216945, -0.07307305, ...,  0.01966017,\n          -0.06616855, -0.00081893],\n         [-0.03514791,  0.04141579,  0.04481114, ...,  0.06427529,\n          -0.05128245, -0.04337732]],\n\n        [[ 0.01960317,  0.08110889, -0.05065569, ...,  0.07411543,\n          -0.02563556,  0.03024834],\n         [ 0.077209  , -0.04263089, -0.06207162, ...,  0.02550242,\n           0.07908883, -0.02494661],\n         [-0.0137411 ,  0.02905273,  0.020271  , ...,  0.0148167 ,\n          -0.0146755 , -0.05974553],\n         ...,\n         [ 0.07827396,  0.02542019, -0.04105077, ...,  0.07301182,\n           0.08015092,  0.07740734],\n         [ 0.03797013,  0.01828101,  0.06534401, ..., -0.01939579,\n           0.04883713, -0.04024935],\n         [-0.01840176,  0.04549287, -0.05745634, ..., -0.01360223,\n          -0.06846356,  0.07146534]],\n\n        [[-0.08174227, -0.04319054,  0.00514207, ..., -0.01205786,\n          -0.03296026,  0.04976875],\n         [ 0.05354258, -0.00556201,  0.0582151 , ..., -0.04334138,\n           0.06577963,  0.02623764],\n         [-0.04761519, -0.02017347, -0.00843507, ..., -0.03588595,\n           0.01288992,  0.06692273],\n         ...,\n         [ 0.00411558,  0.04135039,  0.03604009, ..., -0.01093256,\n           0.01114064,  0.00543308],\n         [-0.03374019, -0.024954  ,  0.02764913, ...,  0.06904659,\n           0.04201163,  0.01444662],\n         [-0.06684017, -0.05146621,  0.06558526, ...,  0.07234574,\n           0.07118727, -0.05247933]]],\n\n\n       [[[ 0.02077695, -0.00187305,  0.03575812, ...,  0.02982893,\n           0.05027277,  0.04850861],\n         [ 0.03044951,  0.06913639, -0.03477621, ..., -0.07878844,\n           0.00798943, -0.04601198],\n         [ 0.06535903,  0.00629401, -0.00410497, ..., -0.05560794,\n           0.0392978 , -0.00123346],\n         ...,\n         [-0.04378315,  0.06720487,  0.01940614, ..., -0.04239967,\n          -0.05567314,  0.03575023],\n         [ 0.04297205, -0.0456691 , -0.02656518, ...,  0.03513962,\n          -0.00659426,  0.05752993],\n         [-0.02449781, -0.07707467, -0.07209063, ...,  0.01041081,\n          -0.04121944,  0.01540587]],\n\n        [[ 0.00215662,  0.0820801 ,  0.01990247, ..., -0.06442132,\n          -0.00513661, -0.04317605],\n         [ 0.05494503, -0.02374178, -0.04593595, ...,  0.02747013,\n           0.02792851, -0.06234119],\n         [-0.02065265, -0.02749845, -0.07035393, ...,  0.03303631,\n          -0.00575292, -0.041046  ],\n         ...,\n         [ 0.02054892,  0.02918879,  0.04967926, ..., -0.00331519,\n           0.032446  , -0.00586434],\n         [ 0.01392857,  0.04845939, -0.06637548, ...,  0.00743705,\n          -0.01400916,  0.08319924],\n         [-0.02172425, -0.01518768,  0.01606623, ..., -0.01281329,\n          -0.05880431,  0.0314281 ]],\n\n        [[ 0.05677726,  0.01288529, -0.08178856, ...,  0.0045661 ,\n          -0.0011861 , -0.04291843],\n         [-0.05863401, -0.07951623, -0.07241204, ...,  0.03616643,\n           0.03830548, -0.07005432],\n         [-0.03403916,  0.0271456 ,  0.01232588, ..., -0.02565426,\n          -0.02148436, -0.07235152],\n         ...,\n         [-0.04798188, -0.01486143,  0.06022414, ...,  0.05292798,\n          -0.0195879 , -0.05546286],\n         [ 0.0536802 , -0.02341622, -0.05619989, ..., -0.02107247,\n           0.01460256, -0.01807783],\n         [ 0.02481482,  0.01606518, -0.05635709, ..., -0.0504626 ,\n          -0.05637823, -0.05407457]]]], dtype=float32)>: ['conv2d_116/kernel']\n    <tf.Variable 'conv2d_116/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_116/bias']\n    <tf.Variable 'batch_normalization_131/gamma:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_131/gamma']\n    <tf.Variable 'batch_normalization_131/beta:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_131/beta']\n    <tf.Variable 'batch_normalization_131/moving_mean:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_131/moving_mean']\n    <tf.Variable 'batch_normalization_131/moving_variance:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_131/moving_variance']\n    <tf.Variable 'conv2d_117/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=\narray([[[[ 0.04438304, -0.04285087,  0.01372197, ..., -0.00011134,\n          -0.03350283,  0.033383  ],\n         [-0.05145435,  0.0066842 ,  0.0524209 , ...,  0.0653877 ,\n          -0.02055708,  0.04530247],\n         [ 0.03368892,  0.02164079, -0.07096566, ..., -0.0069989 ,\n          -0.04764551,  0.06185031],\n         ...,\n         [-0.04843613, -0.01151731, -0.03517066, ...,  0.06599356,\n          -0.05435172, -0.00555785],\n         [-0.0248896 , -0.04914992, -0.04809322, ..., -0.06303603,\n           0.03960167,  0.02678834],\n         [-0.01191647, -0.06993486,  0.04567583, ..., -0.02403647,\n           0.00416996,  0.02237256]],\n\n        [[-0.05666338, -0.06741683,  0.01532263, ...,  0.02284519,\n          -0.06029843,  0.06942776],\n         [-0.06540984,  0.05055628,  0.06774756, ..., -0.06818701,\n          -0.04264463,  0.03898373],\n         [ 0.05039816, -0.06217661, -0.0492875 , ..., -0.04672299,\n           0.04685744, -0.01774649],\n         ...,\n         [ 0.06399538, -0.03546615, -0.02212004, ..., -0.02660194,\n           0.05024142,  0.06244567],\n         [ 0.05032002, -0.03508443, -0.01643254, ..., -0.00415868,\n           0.06462857,  0.00122332],\n         [ 0.02001463,  0.01953696, -0.03275201, ...,  0.03784043,\n           0.03640389,  0.03026084]],\n\n        [[-0.04184863, -0.0085455 , -0.02683187, ...,  0.05295728,\n           0.06530787,  0.07112913],\n         [ 0.03992489, -0.02688261, -0.06522462, ..., -0.04686223,\n           0.04436062,  0.06112383],\n         [-0.03357723, -0.00983307, -0.05401913, ..., -0.03685551,\n          -0.00376391, -0.01079582],\n         ...,\n         [ 0.00457259, -0.07094861,  0.0460915 , ..., -0.05403766,\n           0.03437745, -0.06845114],\n         [-0.04561963,  0.01354801, -0.06585649, ...,  0.01189281,\n           0.02888505, -0.02551439],\n         [ 0.04832169, -0.04616661, -0.06813759, ..., -0.01463365,\n           0.05538391,  0.02185833]]],\n\n\n       [[[-0.04970164, -0.01272995,  0.04075366, ...,  0.03772872,\n          -0.0674525 , -0.06899706],\n         [-0.02783239, -0.00629096,  0.01843622, ...,  0.0132632 ,\n           0.03179472,  0.02738975],\n         [ 0.0435734 , -0.04655456, -0.05144501, ...,  0.03502637,\n          -0.03955234,  0.04637276],\n         ...,\n         [ 0.0121603 ,  0.03119673, -0.03320348, ...,  0.0071391 ,\n          -0.0612465 , -0.00995996],\n         [-0.07110415,  0.05117569, -0.03385988, ..., -0.05418261,\n          -0.0635227 ,  0.05754654],\n         [-0.02510001, -0.00163276, -0.01566654, ..., -0.03747276,\n          -0.00690898,  0.00663006]],\n\n        [[ 0.06959999, -0.04133851,  0.05813232, ..., -0.06369941,\n          -0.0332462 ,  0.00445484],\n         [ 0.02912505, -0.06469519,  0.04489003, ..., -0.00385603,\n           0.02932602, -0.00194353],\n         [ 0.05678529,  0.07034063,  0.0484549 , ..., -0.03730074,\n          -0.05990511, -0.03386389],\n         ...,\n         [ 0.06124784, -0.00156062,  0.05893785, ..., -0.03772413,\n          -0.0540878 , -0.02331681],\n         [ 0.06264238,  0.02784982,  0.0576838 , ..., -0.04768804,\n           0.04428057,  0.04030138],\n         [-0.01988168,  0.0681812 , -0.06251813, ..., -0.0713319 ,\n           0.07074553,  0.03368422]],\n\n        [[ 0.07071967,  0.05504942,  0.02465089, ...,  0.06811538,\n           0.03719527,  0.03165621],\n         [-0.04879937,  0.06996217,  0.06860951, ...,  0.03760716,\n           0.05242384,  0.00094993],\n         [ 0.05609709, -0.06910016, -0.00144058, ..., -0.05156258,\n          -0.03266622,  0.00507005],\n         ...,\n         [-0.03941873, -0.01882354,  0.06653954, ..., -0.02171435,\n          -0.03149492,  0.04557445],\n         [-0.02137103,  0.00311795, -0.04449124, ..., -0.05712911,\n          -0.01872971,  0.00584043],\n         [-0.05034996,  0.02177434, -0.0224377 , ...,  0.01662668,\n          -0.02704531, -0.04708452]]],\n\n\n       [[[ 0.05569963, -0.03258986, -0.00135416, ...,  0.04769374,\n           0.06976673,  0.01427959],\n         [ 0.0194154 , -0.05084968, -0.02335573, ..., -0.03606197,\n           0.02512049,  0.03126741],\n         [ 0.04725768,  0.0075729 , -0.01658427, ..., -0.02292686,\n           0.0249723 , -0.04305361],\n         ...,\n         [ 0.02627002, -0.01923196,  0.04843508, ...,  0.04033419,\n          -0.01041249, -0.04651652],\n         [-0.0032336 , -0.05547963,  0.04287176, ..., -0.04120681,\n           0.06320404,  0.06568711],\n         [ 0.01238833,  0.06561546,  0.04218252, ..., -0.05353834,\n          -0.07088326,  0.06038113]],\n\n        [[ 0.0588329 , -0.06585014,  0.00516375, ...,  0.0631793 ,\n          -0.04019496, -0.00482945],\n         [-0.06247793, -0.05692145,  0.05196715, ..., -0.05612189,\n          -0.01730888,  0.01347722],\n         [ 0.03445052, -0.02037941,  0.05801201, ..., -0.00157129,\n           0.01940921, -0.02310072],\n         ...,\n         [ 0.02310533,  0.03313166,  0.03858825, ...,  0.05428907,\n          -0.01432565,  0.01131889],\n         [-0.06108302,  0.06623052, -0.05047789, ..., -0.03463778,\n          -0.02358206,  0.03197179],\n         [ 0.06087202, -0.06729334, -0.03871019, ...,  0.01125055,\n          -0.03692764,  0.02934371]],\n\n        [[ 0.02823823, -0.05539232, -0.03197594, ..., -0.0659197 ,\n          -0.00187466,  0.03192572],\n         [ 0.01177758,  0.05835094, -0.04251462, ..., -0.00739201,\n          -0.00145199, -0.06548118],\n         [ 0.0012126 ,  0.0084261 ,  0.01720739, ..., -0.00425037,\n          -0.04573406,  0.05145468],\n         ...,\n         [-0.03969051, -0.0525812 , -0.02062179, ..., -0.05824813,\n           0.06520851,  0.03068282],\n         [-0.03640494,  0.01628947, -0.07133237, ..., -0.02125225,\n          -0.06235721, -0.00847837],\n         [ 0.02021034,  0.07002898, -0.02424689, ..., -0.04233089,\n          -0.00221475,  0.06101018]]]], dtype=float32)>: ['conv2d_117/kernel']\n    <tf.Variable 'conv2d_117/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_117/bias']\n    <tf.Variable 'batch_normalization_132/gamma:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_132/gamma']\n    <tf.Variable 'batch_normalization_132/beta:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_132/beta']\n    <tf.Variable 'batch_normalization_132/moving_mean:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_132/moving_mean']\n    <tf.Variable 'batch_normalization_132/moving_variance:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_132/moving_variance']\n    <tf.Variable 'conv2d_118/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=\narray([[[[ 0.03877113,  0.01833312,  0.01893025, ..., -0.05776759,\n          -0.06151442, -0.03601512],\n         [-0.05116709,  0.05850439,  0.01984343, ..., -0.01334621,\n           0.02125169, -0.0350797 ],\n         [-0.00070491, -0.03257178,  0.06816809, ..., -0.03471268,\n          -0.04834292,  0.05908708],\n         ...,\n         [ 0.04942653,  0.06266218, -0.0021069 , ..., -0.00438359,\n          -0.06648469,  0.04549726],\n         [-0.03526551,  0.03443979, -0.04499265, ..., -0.06243909,\n          -0.05706972,  0.06691751],\n         [-0.06174187,  0.03836393, -0.03227536, ..., -0.00280215,\n           0.02770991,  0.05923393]],\n\n        [[ 0.04542252,  0.00117584, -0.01835831, ..., -0.05846206,\n           0.01748011,  0.00476333],\n         [ 0.02568193,  0.07112138,  0.01883388, ..., -0.01682904,\n           0.07093517,  0.02567424],\n         [ 0.0214892 ,  0.06370644,  0.06460989, ..., -0.06226235,\n           0.06367651, -0.04267108],\n         ...,\n         [-0.01231925, -0.03878116, -0.01435659, ..., -0.04112145,\n           0.02065908, -0.05534385],\n         [-0.01011064, -0.06449308,  0.04553054, ..., -0.06459568,\n          -0.00621687, -0.04857603],\n         [-0.0565642 , -0.06821758, -0.03413916, ...,  0.05004583,\n           0.03125704, -0.05878351]],\n\n        [[-0.04761552, -0.06600253, -0.0518624 , ..., -0.01935761,\n          -0.0241734 , -0.02307914],\n         [-0.05798317,  0.0470167 , -0.05382694, ..., -0.02734956,\n           0.03134477,  0.02986907],\n         [ 0.04247741, -0.0155019 , -0.03319821, ..., -0.00305159,\n           0.06144111, -0.05329078],\n         ...,\n         [ 0.04071425, -0.06901441,  0.05070599, ..., -0.06647912,\n          -0.02296019,  0.04623763],\n         [-0.0565888 , -0.05815906,  0.00165781, ..., -0.06593505,\n           0.00515372, -0.06898157],\n         [ 0.01733115,  0.07154359, -0.03129489, ..., -0.02506095,\n           0.02996183, -0.04260403]]],\n\n\n       [[[-0.04516261,  0.0463337 , -0.04536558, ...,  0.06501704,\n           0.00568268, -0.05899135],\n         [ 0.04214638,  0.00988057,  0.00089636, ..., -0.07072169,\n          -0.04104153,  0.02192716],\n         [-0.01770686, -0.05478507, -0.05104627, ...,  0.05898683,\n          -0.06822073, -0.03223513],\n         ...,\n         [ 0.06841086,  0.03417531,  0.0299482 , ..., -0.01076939,\n           0.03749533,  0.00398302],\n         [-0.03629133,  0.02187862, -0.0258222 , ...,  0.05863181,\n          -0.05516243, -0.01502142],\n         [-0.02733949, -0.05090197, -0.01433316, ...,  0.05674313,\n          -0.04189938,  0.01198992]],\n\n        [[ 0.03188954, -0.07039972,  0.05304247, ..., -0.0655705 ,\n          -0.0065212 , -0.06953745],\n         [-0.04539733, -0.00151605, -0.0518778 , ...,  0.07069567,\n           0.02648086,  0.0553602 ],\n         [ 0.0621936 ,  0.03769825,  0.03377232, ...,  0.05826665,\n          -0.06291612, -0.00311076],\n         ...,\n         [-0.00730772,  0.04134034,  0.00104246, ..., -0.02777834,\n           0.04989239,  0.04253239],\n         [-0.03053559, -0.0093063 ,  0.06875573, ...,  0.06139988,\n           0.0020249 , -0.05570515],\n         [ 0.06723271, -0.03812815,  0.00459572, ..., -0.05536314,\n          -0.03794863, -0.02327976]],\n\n        [[-0.04645947,  0.03012734,  0.0161233 , ..., -0.05706478,\n          -0.04067849, -0.01412322],\n         [-0.04957503, -0.02838425, -0.01845252, ..., -0.05008958,\n          -0.06112057, -0.00776485],\n         [-0.00217076, -0.03719087,  0.04201579, ...,  0.06649381,\n          -0.05802302, -0.04026086],\n         ...,\n         [ 0.03809988, -0.01834358, -0.00597712, ..., -0.03900846,\n           0.05055071,  0.03343792],\n         [ 0.00275002,  0.00485256,  0.05018485, ..., -0.06025784,\n           0.04186558,  0.03521121],\n         [ 0.0118692 ,  0.02633362,  0.05148832, ..., -0.06171424,\n          -0.06679129, -0.06939138]]],\n\n\n       [[[-0.0040705 , -0.06354765,  0.01615597, ...,  0.06927787,\n           0.05164745, -0.01483686],\n         [ 0.07025413,  0.05731997, -0.00689374, ...,  0.02061816,\n          -0.07158032, -0.058349  ],\n         [ 0.05394121, -0.0252414 , -0.01950267, ..., -0.04081823,\n          -0.01345436,  0.018637  ],\n         ...,\n         [-0.06631189, -0.00459479,  0.0194456 , ...,  0.00292176,\n           0.01898488,  0.01096507],\n         [ 0.05909352,  0.05787852, -0.04479546, ..., -0.07010835,\n          -0.00232496,  0.04265188],\n         [-0.01702597,  0.02992214,  0.01510631, ...,  0.04920357,\n          -0.01130041, -0.06844547]],\n\n        [[-0.06974315, -0.05750525,  0.05637971, ...,  0.04629767,\n          -0.01945687,  0.06011067],\n         [-0.00213712,  0.00575739, -0.06786154, ...,  0.01156081,\n          -0.00796626,  0.03657601],\n         [ 0.00955804,  0.00554314,  0.03028002, ..., -0.06273021,\n           0.03617402, -0.00306208],\n         ...,\n         [-0.00913791,  0.05858643, -0.0342128 , ..., -0.02685412,\n          -0.00211862,  0.05895548],\n         [-0.04518333, -0.02263317, -0.061915  , ..., -0.03589646,\n          -0.04154506, -0.04066733],\n         [-0.05078237,  0.03219026, -0.01999066, ...,  0.00875963,\n          -0.04741736, -0.05800588]],\n\n        [[-0.03001233, -0.01149807, -0.01185129, ..., -0.06561344,\n           0.00220028, -0.0018803 ],\n         [ 0.04597792,  0.00784519, -0.01591411, ..., -0.00528041,\n           0.04508949,  0.01272945],\n         [-0.03790894,  0.03259083, -0.0264221 , ..., -0.05438966,\n           0.00305047,  0.00836204],\n         ...,\n         [-0.03962624,  0.05532111,  0.00554682, ...,  0.01193276,\n           0.03864805,  0.0569544 ],\n         [ 0.03286623,  0.03293015,  0.04660929, ...,  0.03876019,\n          -0.04701607, -0.04473887],\n         [ 0.03670609, -0.03495495,  0.02037287, ..., -0.05017339,\n          -0.0045495 , -0.00400575]]]], dtype=float32)>: ['conv2d_118/kernel']\n    <tf.Variable 'conv2d_118/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_118/bias']\n    <tf.Variable 'batch_normalization_133/gamma:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_133/gamma']\n    <tf.Variable 'batch_normalization_133/beta:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_133/beta']\n    <tf.Variable 'batch_normalization_133/moving_mean:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_133/moving_mean']\n    <tf.Variable 'batch_normalization_133/moving_variance:0' shape=(64,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_133/moving_variance']\n    <tf.Variable 'conv2d_119/kernel:0' shape=(3, 3, 64, 128) dtype=float32, numpy=\narray([[[[ 5.92639670e-03,  3.67051177e-02, -3.15407291e-03, ...,\n          -1.11163780e-03,  4.50912006e-02, -9.13154334e-04],\n         [-4.50183973e-02,  3.69103439e-02, -4.35523614e-02, ...,\n           2.20872276e-02, -8.50735605e-03,  6.83987513e-03],\n         [ 2.07550824e-03,  2.06881873e-02, -7.99454749e-04, ...,\n           1.69167928e-02,  4.24238667e-03,  3.15575786e-02],\n         ...,\n         [-4.20955569e-03,  1.03060119e-02, -3.34499329e-02, ...,\n           3.75226401e-02,  2.95442343e-03,  9.13659111e-03],\n         [ 5.06296642e-02,  6.05051592e-03, -1.44848451e-02, ...,\n          -4.75230999e-02,  7.66558573e-03, -4.68068868e-02],\n         [-2.62558460e-05,  2.77219601e-02,  5.84110953e-02, ...,\n           5.63734062e-02,  5.09757437e-02,  4.89783473e-02]],\n\n        [[-1.58014558e-02, -5.27883396e-02, -4.35903780e-02, ...,\n           1.11281760e-02,  5.47892340e-02,  1.55311115e-02],\n         [ 6.07189909e-03, -1.85826868e-02,  2.27298550e-02, ...,\n          -3.09677627e-02, -4.90207039e-02, -9.70738754e-03],\n         [-1.71526857e-02,  2.70762667e-03,  4.96365540e-02, ...,\n           3.61494906e-02,  1.86730362e-02, -5.29034138e-02],\n         ...,\n         [ 2.29848139e-02,  3.51279639e-02,  3.34672369e-02, ...,\n           1.85861103e-02, -2.05486268e-02, -5.01186401e-03],\n         [-5.45427725e-02,  4.56480049e-02,  4.66168486e-02, ...,\n           3.24751027e-02,  1.22028776e-02, -8.99219885e-03],\n         [ 1.51127614e-02,  5.80065139e-02, -3.74097116e-02, ...,\n          -2.66156644e-02, -2.65194289e-02,  6.47733733e-03]],\n\n        [[ 5.05826510e-02,  1.32703967e-02,  4.88886870e-02, ...,\n           1.63400136e-02, -2.91801747e-02,  1.37164108e-02],\n         [ 4.86677103e-02,  2.19557434e-04,  5.58342896e-02, ...,\n          -2.69679688e-02,  4.29415964e-02,  4.83439490e-03],\n         [ 2.55890377e-02, -5.10364920e-02,  2.58725509e-03, ...,\n           1.68920495e-02,  2.78913118e-02, -5.74136302e-02],\n         ...,\n         [ 2.68697366e-03,  3.38242762e-02,  1.18476935e-02, ...,\n           5.05354889e-02,  9.12024453e-03,  3.82690690e-02],\n         [-3.41674685e-02, -2.05314308e-02,  1.84942745e-02, ...,\n          -2.25872844e-02, -5.58495335e-02,  3.38141806e-02],\n         [ 7.25823268e-03, -2.69127153e-02, -8.69541615e-03, ...,\n           2.81900205e-02, -2.23457813e-02,  1.59018375e-02]]],\n\n\n       [[[ 2.32915953e-03,  2.94929035e-02,  3.72573622e-02, ...,\n           5.55808060e-02, -2.74109095e-03, -5.54106012e-02],\n         [ 4.68080081e-02,  2.01425292e-02,  4.59944643e-02, ...,\n          -5.16686663e-02, -3.53650004e-02,  3.48044597e-02],\n         [ 4.30017300e-02, -4.46912386e-02,  9.46910307e-03, ...,\n          -5.29691465e-02, -2.65936516e-02, -5.22887856e-02],\n         ...,\n         [ 4.96440418e-02,  5.63724674e-02, -1.36712417e-02, ...,\n           1.03393681e-02, -6.24717399e-03, -1.06401816e-02],\n         [ 3.34173627e-02, -2.57701986e-02, -2.08226591e-03, ...,\n          -3.81672308e-02,  3.72742377e-02,  5.84307052e-02],\n         [-3.05254310e-02, -1.41575336e-02,  4.63289432e-02, ...,\n           3.11797075e-02,  1.77289434e-02,  4.10461016e-02]],\n\n        [[-5.10600358e-02,  5.42509072e-02, -6.89703971e-04, ...,\n          -3.79039682e-02,  4.29699905e-02, -4.30909805e-02],\n         [-1.99785493e-02,  3.52671631e-02, -5.61943352e-02, ...,\n          -4.01885808e-02,  5.51172718e-03,  2.66741626e-02],\n         [-7.04827532e-03,  5.85093908e-02,  4.21230905e-02, ...,\n          -2.34619565e-02, -5.77718094e-02, -1.34788863e-02],\n         ...,\n         [-2.54554451e-02, -3.55947986e-02, -2.30057724e-02, ...,\n           5.14299758e-02, -1.77750550e-02, -5.36220446e-02],\n         [ 2.16647200e-02, -5.85373491e-02,  4.51355241e-02, ...,\n          -3.84649709e-02, -3.69175486e-02,  3.43742706e-02],\n         [-3.27188820e-02,  2.53531672e-02,  2.20822506e-02, ...,\n           5.07080369e-02, -2.83188745e-03, -1.01960115e-02]],\n\n        [[ 2.54969187e-02, -2.61611827e-02, -3.48166674e-02, ...,\n          -2.54312530e-02,  5.35753742e-03, -5.37595563e-02],\n         [-4.86933775e-02,  3.71298455e-02, -3.68448570e-02, ...,\n           9.54554603e-03, -7.39664584e-03,  1.31798498e-02],\n         [-3.83893475e-02, -5.46623841e-02, -4.05137986e-02, ...,\n          -5.30320741e-02,  1.22077055e-02,  4.72660176e-02],\n         ...,\n         [ 1.29704960e-02,  4.89976518e-02, -5.29685430e-02, ...,\n           7.35074654e-03, -2.63867527e-03,  8.54104385e-03],\n         [ 1.50210224e-02, -2.04025321e-02, -5.77369109e-02, ...,\n          -2.10241675e-02, -4.34633046e-02,  2.73389779e-02],\n         [-1.78920813e-02, -2.34989896e-02,  8.72182474e-03, ...,\n          -3.17422822e-02, -2.78989654e-02,  5.10256849e-02]]],\n\n\n       [[[ 5.16392477e-02,  5.61533310e-02,  9.08097252e-03, ...,\n           3.08794193e-02,  3.93535532e-02,  2.49830373e-02],\n         [ 2.79930644e-02, -4.14737687e-03, -1.11454092e-02, ...,\n           5.03407307e-02,  4.47459519e-04,  5.27900942e-02],\n         [ 3.52340564e-03, -7.22824037e-03,  2.77921930e-03, ...,\n          -4.80273589e-02, -5.18609397e-02, -2.40061283e-02],\n         ...,\n         [-2.89626047e-03,  4.55319025e-02,  4.90672477e-02, ...,\n          -5.69268353e-02, -5.39752766e-02,  5.38167246e-02],\n         [-2.30558030e-02,  3.07509862e-02,  4.74419408e-02, ...,\n          -4.16974649e-02, -2.29245052e-03,  1.77171864e-02],\n         [ 2.91716047e-02, -3.01435813e-02,  2.47952156e-02, ...,\n           2.62738951e-02,  1.46752112e-02, -3.56252976e-02]],\n\n        [[-1.55641846e-02, -2.96125300e-02,  1.23685263e-02, ...,\n          -5.72897904e-02,  5.09961434e-02,  5.38648926e-02],\n         [ 3.18529569e-02,  1.37805454e-02, -5.22247329e-03, ...,\n           4.17382084e-02, -3.17032672e-02, -9.25127417e-03],\n         [ 3.49958055e-02, -5.74504249e-02,  8.70924070e-03, ...,\n           4.63348962e-02, -1.20525807e-02,  5.52913807e-02],\n         ...,\n         [ 6.64596632e-03,  1.68264993e-02,  3.08705010e-02, ...,\n          -2.55147889e-02, -5.43857738e-02, -1.58110671e-02],\n         [-1.64798275e-03,  3.11265327e-02,  3.31513323e-02, ...,\n          -2.87190080e-04,  3.46353538e-02, -7.44935870e-03],\n         [-4.88969311e-03, -1.58995464e-02, -2.78077181e-02, ...,\n          -1.58512741e-02,  2.64122896e-02,  3.70878763e-02]],\n\n        [[ 5.39955869e-03,  5.30493669e-02, -4.24428172e-02, ...,\n          -1.84869990e-02,  3.90446968e-02,  3.97465415e-02],\n         [-4.13938090e-02,  2.99666934e-02, -5.67562245e-02, ...,\n          -8.98604468e-03,  2.97341831e-02,  1.71240084e-02],\n         [-2.29456425e-02, -2.82913670e-02,  4.04812060e-02, ...,\n           2.04350837e-02,  5.27900942e-02,  2.19430663e-02],\n         ...,\n         [-1.20399520e-03,  2.91409902e-02,  3.82960625e-02, ...,\n          -1.47794262e-02,  2.14670785e-02, -2.18186229e-02],\n         [ 3.92224230e-02, -4.78372350e-02,  4.43377718e-03, ...,\n           4.08553444e-02, -3.97521928e-02,  4.30302210e-02],\n         [ 5.45667671e-02, -2.98143439e-02,  4.19466384e-02, ...,\n          -5.21018058e-02,  4.69476990e-02, -2.58032978e-02]]]],\n      dtype=float32)>: ['conv2d_119/kernel']\n    <tf.Variable 'conv2d_119/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_119/bias']\n    <tf.Variable 'batch_normalization_134/gamma:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_134/gamma']\n    <tf.Variable 'batch_normalization_134/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_134/beta']\n    <tf.Variable 'batch_normalization_134/moving_mean:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_134/moving_mean']\n    <tf.Variable 'batch_normalization_134/moving_variance:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_134/moving_variance']\n    <tf.Variable 'conv2d_120/kernel:0' shape=(3, 3, 128, 128) dtype=float32, numpy=\narray([[[[ 4.93689999e-02,  9.58705693e-03,  4.25749421e-02, ...,\n           3.30835804e-02, -2.95136832e-02, -3.75214592e-02],\n         [-3.58970277e-02,  2.49863043e-02, -4.61151861e-02, ...,\n          -2.90487669e-02, -2.40574311e-02, -2.79105920e-02],\n         [ 4.20482159e-02,  3.01028565e-02,  2.46394128e-02, ...,\n          -4.49959897e-02,  1.32843852e-02,  4.52577695e-02],\n         ...,\n         [ 3.18974257e-04, -3.88700329e-02,  3.98853868e-02, ...,\n           7.24281743e-03,  3.16824913e-02, -2.71189753e-02],\n         [-2.23529916e-02, -7.32007623e-03, -4.96918336e-02, ...,\n           1.08009204e-02,  1.51445121e-02, -4.40491624e-02],\n         [-4.56182696e-02,  1.54934675e-02,  2.23518386e-02, ...,\n           4.82646376e-02,  2.31531113e-02, -4.55476716e-03]],\n\n        [[-4.38416824e-02, -9.82550159e-03,  4.86429855e-02, ...,\n           2.38651335e-02, -3.89891192e-02,  2.98683196e-02],\n         [-2.24120170e-03, -4.72485907e-02, -4.87625599e-02, ...,\n           4.59216833e-02,  1.60954967e-02,  1.05360150e-02],\n         [ 1.85889751e-03,  7.11571425e-04,  4.36385795e-02, ...,\n           4.85280305e-02,  3.53305712e-02,  9.72882286e-03],\n         ...,\n         [ 9.25891846e-03,  2.97392085e-02,  4.97144535e-02, ...,\n           1.90239772e-02,  1.78387761e-03,  1.01246685e-03],\n         [-1.95267797e-03, -1.62639990e-02,  3.62802669e-03, ...,\n          -2.50708610e-02, -1.51998438e-02,  1.76016241e-03],\n         [-4.98890132e-03, -2.12633386e-02, -1.01318695e-02, ...,\n          -2.78186835e-02, -6.25446439e-03,  9.86419618e-04]],\n\n        [[ 1.12398379e-02, -2.65389010e-02, -3.77460942e-02, ...,\n          -2.19314378e-02, -4.59022447e-02,  7.18642026e-04],\n         [ 3.50316465e-02,  4.11315411e-02, -3.09306215e-02, ...,\n           1.39028579e-02,  4.45092320e-02,  9.12372023e-04],\n         [-4.39234190e-02, -2.98041645e-02,  3.71468291e-02, ...,\n          -2.29838379e-02,  3.17297801e-02,  8.14467669e-03],\n         ...,\n         [-4.49046791e-02,  3.12201455e-02,  2.32214183e-02, ...,\n          -3.26759890e-02, -8.52194428e-03, -2.56756581e-02],\n         [ 2.93649957e-02, -4.58571762e-02,  2.63534039e-02, ...,\n           2.88554132e-02, -3.29503268e-02,  4.29437980e-02],\n         [ 2.12391838e-02,  2.14958340e-02, -2.33347509e-02, ...,\n           2.44273990e-02, -4.46736068e-03,  1.46632269e-03]]],\n\n\n       [[[-3.78015973e-02,  2.49770805e-02,  3.30687314e-03, ...,\n          -3.56002524e-03,  2.36704946e-02,  3.55348140e-02],\n         [-3.95305157e-02,  1.58600956e-02,  3.73867452e-02, ...,\n           5.20486012e-03, -3.64764929e-02, -1.48677155e-02],\n         [ 3.40450928e-02, -4.67161387e-02, -2.84752864e-02, ...,\n           1.30275786e-02,  3.03959325e-02, -2.61642020e-02],\n         ...,\n         [-6.10230491e-03, -2.25794148e-02, -3.66666839e-02, ...,\n          -5.08478917e-02,  5.09331599e-02, -1.99583694e-02],\n         [ 4.97555509e-02,  2.59329379e-02, -9.17588174e-03, ...,\n          -1.17883086e-04,  4.51394096e-02, -1.10434666e-02],\n         [ 2.85531655e-02,  1.90091655e-02,  2.63330787e-02, ...,\n           2.64201686e-02, -1.40645653e-02,  2.44300738e-02]],\n\n        [[-2.90244333e-02, -4.46649455e-02, -1.38321146e-03, ...,\n           2.27538049e-02,  4.43653166e-02, -2.98210513e-02],\n         [ 1.35339946e-02,  1.60827488e-02,  4.07807529e-03, ...,\n          -1.65734477e-02,  4.17180732e-03,  1.26213804e-02],\n         [-1.13680474e-02,  1.07784234e-02,  4.26524431e-02, ...,\n          -2.97750905e-03,  4.08553556e-02, -8.91075656e-03],\n         ...,\n         [-4.65830192e-02,  4.95095253e-02,  9.83839855e-03, ...,\n          -2.24753041e-02,  6.30866364e-03, -3.80909219e-02],\n         [ 4.10273224e-02, -5.23339212e-03,  2.23615691e-02, ...,\n           4.39599156e-03, -3.84214930e-02,  4.29538637e-02],\n         [-3.03713605e-03, -5.09993210e-02, -3.67709771e-02, ...,\n          -3.27449031e-02,  3.24407816e-02, -1.15534961e-02]],\n\n        [[ 3.90858129e-02,  1.09954923e-02,  3.10870111e-02, ...,\n           3.27078104e-02, -3.77341099e-02, -3.95357236e-02],\n         [ 3.35696489e-02,  4.06949595e-02, -1.76205523e-02, ...,\n          -4.97529693e-02,  2.10944861e-02, -3.40397507e-02],\n         [ 3.40584889e-02, -1.98929124e-02,  4.96087670e-02, ...,\n          -1.25674345e-02, -3.61455940e-02,  1.28065944e-02],\n         ...,\n         [-7.88710639e-03, -2.65663862e-02, -9.10404697e-03, ...,\n           3.00718173e-02, -3.55612859e-02,  2.65908614e-02],\n         [-4.98792119e-02,  6.97442889e-03, -1.65928900e-02, ...,\n          -4.17901017e-02,  4.37593386e-02, -2.14014202e-03],\n         [ 7.22844899e-03, -2.80509591e-02, -2.40052603e-02, ...,\n          -3.62379514e-02,  1.04710199e-02, -4.58389036e-02]]],\n\n\n       [[[ 4.45516035e-03, -4.04862650e-02, -1.11799501e-02, ...,\n           6.19007647e-03, -4.32470217e-02,  1.09133124e-03],\n         [-3.40234339e-02,  4.19235602e-02, -3.68703417e-02, ...,\n          -3.93697694e-02, -1.51001886e-02,  3.58697847e-02],\n         [ 1.18510798e-02,  7.32588023e-03,  2.50797570e-02, ...,\n          -3.77715081e-02,  2.50317305e-02,  2.89539769e-02],\n         ...,\n         [-2.16627605e-02,  2.85937786e-02, -4.73601110e-02, ...,\n          -3.69840041e-02,  2.47238129e-02, -4.06035781e-02],\n         [-2.33545583e-02,  2.13541389e-02,  1.81413144e-02, ...,\n          -1.07173100e-02, -4.58678119e-02,  3.59933674e-02],\n         [-3.20849046e-02, -4.51635569e-03, -4.44968417e-03, ...,\n          -1.88283809e-02,  1.18447915e-02,  4.98355553e-02]],\n\n        [[ 5.53007796e-03, -7.56491721e-03,  4.83432934e-02, ...,\n           1.61642432e-02,  1.29085332e-02, -4.67260890e-02],\n         [ 4.00211662e-03, -2.16754861e-02, -2.67323889e-02, ...,\n          -1.81723647e-02,  3.01630944e-02,  9.04835761e-05],\n         [ 4.37090173e-02, -1.19524896e-02, -4.73576039e-03, ...,\n           1.43546313e-02, -2.46655252e-02,  7.38089532e-03],\n         ...,\n         [-4.58623730e-02, -1.06066167e-02,  1.61216706e-02, ...,\n           2.01153010e-03,  3.18694338e-02, -5.03282622e-02],\n         [ 2.63516083e-02, -1.08181685e-03,  4.58064675e-02, ...,\n          -4.90407646e-03, -4.23228815e-02,  4.24849615e-03],\n         [ 1.68408081e-02, -4.53864932e-02, -4.39598709e-02, ...,\n          -2.80823261e-02,  1.74742714e-02, -4.84034084e-02]],\n\n        [[ 4.85124961e-02, -1.16197802e-02, -2.75778435e-02, ...,\n          -5.03165349e-02,  2.17136890e-02,  3.81449908e-02],\n         [-5.25261462e-03,  2.86215767e-02, -1.68121085e-02, ...,\n          -1.50543563e-02,  2.64202282e-02, -1.51182301e-02],\n         [-3.15402001e-02, -4.67997342e-02, -3.10818423e-02, ...,\n           4.74980846e-02, -2.22491845e-02, -1.44137405e-02],\n         ...,\n         [ 2.65321508e-03, -4.60932404e-03, -3.35533619e-02, ...,\n           3.27999368e-02,  2.38845795e-02, -3.95544730e-02],\n         [-3.97529379e-02, -4.67099585e-02, -7.71218538e-03, ...,\n           3.67036834e-03, -2.24653762e-02,  2.25502625e-02],\n         [-4.01150584e-02, -1.11637712e-02,  2.60866359e-02, ...,\n          -3.43745053e-02, -3.20409350e-02, -4.44150269e-02]]]],\n      dtype=float32)>: ['conv2d_120/kernel']\n    <tf.Variable 'conv2d_120/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_120/bias']\n    <tf.Variable 'batch_normalization_135/gamma:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_135/gamma']\n    <tf.Variable 'batch_normalization_135/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_135/beta']\n    <tf.Variable 'batch_normalization_135/moving_mean:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_135/moving_mean']\n    <tf.Variable 'batch_normalization_135/moving_variance:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_135/moving_variance']\n    <tf.Variable 'conv2d_121/kernel:0' shape=(3, 3, 128, 128) dtype=float32, numpy=\narray([[[[-0.01269745,  0.04905408,  0.02013322, ..., -0.00239365,\n           0.04028796,  0.0506104 ],\n         [ 0.01761603, -0.00885704, -0.03194425, ...,  0.02918463,\n           0.02075962,  0.01567951],\n         [-0.00478531, -0.02705448, -0.0421623 , ...,  0.04461244,\n          -0.00194044,  0.02606156],\n         ...,\n         [-0.02198179,  0.04070072,  0.00429545, ..., -0.04405405,\n          -0.01480733,  0.03806011],\n         [-0.00807586, -0.01942381, -0.00185722, ..., -0.03168061,\n          -0.00117967, -0.04921336],\n         [ 0.02640632, -0.0175267 , -0.04042577, ..., -0.03872542,\n           0.01161586, -0.03686703]],\n\n        [[-0.0187618 ,  0.02627771, -0.01435858, ..., -0.00417582,\n          -0.03776024,  0.04830144],\n         [ 0.00759704, -0.0006447 , -0.00274377, ...,  0.02343971,\n           0.02065577,  0.05051273],\n         [ 0.02344444,  0.02949789,  0.04057142, ..., -0.02869102,\n           0.04550803,  0.04740167],\n         ...,\n         [-0.00709274,  0.0081076 , -0.02629811, ..., -0.01269975,\n           0.02290613, -0.03631336],\n         [ 0.03634275, -0.01124256,  0.04328647, ...,  0.04003695,\n           0.00758653,  0.02584406],\n         [ 0.01550908, -0.05078228,  0.02146696, ..., -0.01670195,\n           0.01546928, -0.02428437]],\n\n        [[-0.03146384, -0.0006202 , -0.00566981, ..., -0.02294147,\n           0.00487546,  0.0480549 ],\n         [ 0.01846618, -0.02987513, -0.00930543, ..., -0.00318329,\n           0.00564116,  0.0497755 ],\n         [ 0.02111596,  0.02783251,  0.00626739, ...,  0.04759048,\n          -0.04127146,  0.01434208],\n         ...,\n         [ 0.02997809,  0.03887384, -0.04013238, ...,  0.01862207,\n           0.0119507 ,  0.04134411],\n         [ 0.01634627, -0.01655083,  0.04111227, ..., -0.02278326,\n           0.00080507, -0.05036426],\n         [-0.04078617,  0.02974917,  0.03301825, ..., -0.03375012,\n           0.03513197,  0.00281435]]],\n\n\n       [[[ 0.01391927, -0.02485917, -0.00090578, ...,  0.01682666,\n           0.02359542, -0.03122708],\n         [ 0.03192087, -0.02321855, -0.03504992, ..., -0.01961511,\n          -0.02472321,  0.00728473],\n         [ 0.01709327,  0.01605997,  0.03525992, ..., -0.01185247,\n           0.03889409,  0.0065316 ],\n         ...,\n         [-0.03872247, -0.00047392, -0.01674691, ...,  0.01983491,\n           0.00376344,  0.03968523],\n         [-0.00358749, -0.00097374,  0.02656883, ..., -0.03613694,\n          -0.01210178,  0.02947091],\n         [ 0.01258393,  0.01494206, -0.00203557, ...,  0.00016277,\n           0.02248247,  0.02864754]],\n\n        [[-0.01344074, -0.03173933, -0.00285281, ..., -0.04052752,\n           0.02927332,  0.02395136],\n         [ 0.01883733,  0.02292868, -0.01404168, ...,  0.0254643 ,\n          -0.01838596, -0.05001246],\n         [ 0.0052635 , -0.02194884, -0.01074829, ..., -0.00113347,\n          -0.01635316, -0.03385675],\n         ...,\n         [-0.00742848, -0.03546091,  0.04538983, ..., -0.03409924,\n           0.02153142,  0.0117134 ],\n         [ 0.03403291, -0.03202695,  0.01263048, ...,  0.03924796,\n          -0.01793033,  0.00832601],\n         [ 0.00595679, -0.0270025 , -0.03028694, ..., -0.03756505,\n           0.02006119, -0.02582492]],\n\n        [[-0.03894284, -0.01346277,  0.00651019, ...,  0.01963861,\n          -0.02323625,  0.04824764],\n         [ 0.00302097,  0.03847945, -0.0412301 , ..., -0.01259196,\n           0.00335873, -0.02051777],\n         [-0.02523535,  0.0223385 ,  0.02383216, ...,  0.01814397,\n           0.03761009, -0.00876028],\n         ...,\n         [-0.02082148, -0.04611816, -0.01451302, ...,  0.02439242,\n          -0.01394914, -0.00135339],\n         [-0.01188872,  0.04591449, -0.01310108, ...,  0.01864554,\n          -0.04902067, -0.03575729],\n         [-0.02070069,  0.00817444, -0.04962518, ..., -0.02482628,\n           0.03065899,  0.00570758]]],\n\n\n       [[[-0.02456442,  0.04740629, -0.04398212, ..., -0.05102671,\n          -0.0306594 , -0.04689203],\n         [-0.0131002 ,  0.02947746, -0.00924108, ...,  0.0143764 ,\n           0.01009108,  0.00772994],\n         [ 0.0045082 , -0.01006932, -0.04107497, ..., -0.00921172,\n          -0.03138926,  0.03753503],\n         ...,\n         [-0.0107894 , -0.01770936,  0.04865663, ..., -0.0365194 ,\n          -0.03847918,  0.02261656],\n         [-0.02107301,  0.00532807,  0.0497468 , ...,  0.03633555,\n          -0.03756252, -0.03185545],\n         [-0.04212245, -0.00360507, -0.02819018, ...,  0.00015112,\n          -0.04969614,  0.0300298 ]],\n\n        [[ 0.00645138,  0.02114152, -0.03259349, ...,  0.01052893,\n          -0.01070336,  0.02749782],\n         [-0.03409984,  0.0390601 ,  0.04490251, ...,  0.03037921,\n           0.01294971,  0.03409515],\n         [-0.04244926,  0.00166082,  0.03609452, ...,  0.02373201,\n           0.0492832 ,  0.00072695],\n         ...,\n         [-0.02338204, -0.00268789,  0.02987336, ..., -0.03632745,\n           0.00079149, -0.03510767],\n         [ 0.00861334,  0.03585543, -0.02030888, ...,  0.0505441 ,\n           0.03711492,  0.01128595],\n         [-0.03462014, -0.01242814, -0.02311822, ...,  0.0311583 ,\n          -0.04677533,  0.02265301]],\n\n        [[-0.0216049 , -0.04351216,  0.0123985 , ...,  0.02953226,\n           0.00624528, -0.01837631],\n         [ 0.04260097, -0.04299523, -0.0477579 , ..., -0.0012338 ,\n          -0.02355404,  0.0407764 ],\n         [ 0.02354145,  0.03925131, -0.03645778, ..., -0.02566555,\n          -0.01228136,  0.01681481],\n         ...,\n         [-0.04246598, -0.04090672, -0.02261275, ...,  0.0303967 ,\n           0.02297536, -0.02888449],\n         [ 0.02143775, -0.0424908 , -0.00735485, ..., -0.02875853,\n           0.00245093,  0.00794704],\n         [ 0.02735361,  0.03793734, -0.0091785 , ..., -0.02265177,\n           0.01899183,  0.02803575]]]], dtype=float32)>: ['conv2d_121/kernel']\n    <tf.Variable 'conv2d_121/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['conv2d_121/bias']\n    <tf.Variable 'batch_normalization_136/gamma:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_136/gamma']\n    <tf.Variable 'batch_normalization_136/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_136/beta']\n    <tf.Variable 'batch_normalization_136/moving_mean:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['batch_normalization_136/moving_mean']\n    <tf.Variable 'batch_normalization_136/moving_variance:0' shape=(128,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>: ['batch_normalization_136/moving_variance']\n    <tf.Variable 'dense_28/kernel:0' shape=(1152, 512) dtype=float32, numpy=\narray([[ 0.04191189,  0.0258666 ,  0.03149901, ...,  0.0442818 ,\n         0.04320597, -0.05236223],\n       [-0.02453576, -0.03397398,  0.05923217, ...,  0.01800971,\n        -0.03894537,  0.04898032],\n       [-0.03726929, -0.042066  , -0.02925297, ..., -0.01210067,\n        -0.03130576, -0.04250278],\n       ...,\n       [-0.04689163, -0.00256719,  0.04955404, ...,  0.05426704,\n        -0.03422888, -0.00604672],\n       [-0.01215716, -0.04747006,  0.00140962, ...,  0.01653213,\n        -0.02344661,  0.03255607],\n       [ 0.01943045,  0.0548328 ,  0.02283884, ..., -0.03332441,\n        -0.02933393,  0.03022568]], dtype=float32)>: ['dense_28/kernel']\n    <tf.Variable 'dense_28/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>: ['dense_28/bias']\n    <tf.Variable 'batch_normalization_137/gamma:0' shape=(512,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1.], dtype=float32)>: ['batch_normalization_137/gamma']\n    <tf.Variable 'batch_normalization_137/beta:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>: ['batch_normalization_137/beta']\n    <tf.Variable 'batch_normalization_137/moving_mean:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>: ['batch_normalization_137/moving_mean']\n    <tf.Variable 'batch_normalization_137/moving_variance:0' shape=(512,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1.], dtype=float32)>: ['batch_normalization_137/moving_variance']\n    <tf.Variable 'dense_29/kernel:0' shape=(512, 26) dtype=float32, numpy=\narray([[ 0.04909568,  0.00680554, -0.01343896, ...,  0.05586115,\n         0.09758975,  0.06862874],\n       [ 0.04505733,  0.09201207,  0.08753008, ...,  0.09166736,\n         0.00117949, -0.02747767],\n       [-0.05488635, -0.02415103, -0.02770717, ..., -0.02085422,\n         0.05624916, -0.05178185],\n       ...,\n       [-0.0919565 , -0.01380374, -0.09338132, ...,  0.02809937,\n        -0.03730787,  0.0370912 ],\n       [-0.04200146,  0.01515886, -0.00024891, ..., -0.05664562,\n         0.06770723, -0.02381963],\n       [ 0.02605527, -0.08068967,  0.05206772, ..., -0.05010242,\n        -0.00716743,  0.05964593]], dtype=float32)>: ['dense_29/kernel']\n    <tf.Variable 'dense_29/bias:0' shape=(26,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['dense_29/bias']"
     ]
    }
   ],
   "source": [
    "model2 = my_model(28,28,1)\n",
    "model2.load_weights('cp.ckpt.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 12  2  8  3  6 14 17 24 16 25 19 19 22 14 14 12 16 12  5  2 17  4 16\n",
      "  3]\n",
      "[9, 12, 2, 8, 3, 6, 14, 17, 24, 16, 25, 19, 19, 22, 14, 14, 12, 16, 12, 5, 2, 17, 4, 16, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JPG\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.predict_classes(X_test[:25]))\n",
    "print(max_index(y_test[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
